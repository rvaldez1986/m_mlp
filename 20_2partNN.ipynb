{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1232)\n",
    "os.chdir('C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\Data\\\\dep')\n",
    "data = pd.read_csv(\"Rand_train.csv\", sep=',')\n",
    "data_val = pd.read_csv(\"Rand_valid.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cost = (np.exp(data.cost) - 0.5)*((data.cost != 0) * 1)\n",
    "data_val.cost = (np.exp(data_val.cost) - 0.5)*((data_val.cost != 0) * 1)\n",
    "data = shuffle(data).reset_index(drop=True)\n",
    "data = data.drop(['fmde', 'lxghindx', 'lpi'], axis=1)\n",
    "data_val = data_val.drop(['fmde', 'lxghindx', 'lpi'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:]\n",
    "Y = data.iloc[:,0]\n",
    "X_val = data_val.iloc[:,1:]\n",
    "Y_val = data_val.iloc[:,0]\n",
    "\n",
    "ranking = np.load('ranking.npy')\n",
    "\n",
    "my_index = X.columns.values[ranking]\n",
    "X2 = X[my_index]\n",
    "X2_val = X_val[my_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_p = (Y == 0) * 1\n",
    "Y_valp = (Y_val == 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\prog_calc')\n",
    "from fit4_nn import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, initial, final):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(initial, final)\n",
    "        self.fc2 = nn.Linear(final, 1)                \n",
    "                   \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = X2.shape[1]\n",
    "final = int(round(initial * 1.5, 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 0.01\n",
    "verbose = 1\n",
    "n_batches = 6\n",
    "batch_to_avg = 2\n",
    "clipping = 0.25\n",
    "PATH = 'C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\Data\\\\checkpoints\\\\mytraining1.pt'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rober\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py:2016: UserWarning: Using a target size (torch.Size([1073])) that is different to the input size (torch.Size([1073, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "c:\\users\\rober\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py:2016: UserWarning: Using a target size (torch.Size([1004])) that is different to the input size (torch.Size([1004, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.7827579279740652, Validation Loss: 0.7592731714248657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2: Training Loss: 0.7483778993288676, Validation Loss: 0.7088706493377686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3: Training Loss: 0.6945424179236094, Validation Loss: 0.6484577059745789\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4: Training Loss: 0.6339693864186605, Validation Loss: 0.5904793739318848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5: Training Loss: 0.5790773630142212, Validation Loss: 0.5430746078491211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 6: Training Loss: 0.5369208951791128, Validation Loss: 0.5102515816688538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 7: Training Loss: 0.5096871505180994, Validation Loss: 0.49295997619628906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 8: Training Loss: 0.49854353070259094, Validation Loss: 0.4890305697917938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 9: Training Loss: 0.49637646476427716, Validation Loss: 0.4893205463886261\n",
      "Epoch 10: Training Loss: 0.4957799017429352, Validation Loss: 0.486554890871048\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 11: Training Loss: 0.49122827251752216, Validation Loss: 0.4803285002708435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 12: Training Loss: 0.48377250134944916, Validation Loss: 0.4733037054538727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 13: Training Loss: 0.47645239035288495, Validation Loss: 0.46804308891296387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 14: Training Loss: 0.4699067324399948, Validation Loss: 0.4631592333316803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 15: Training Loss: 0.46348439157009125, Validation Loss: 0.4563761353492737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 16: Training Loss: 0.45584916075070697, Validation Loss: 0.448001503944397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 17: Training Loss: 0.44734784960746765, Validation Loss: 0.4398738145828247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 18: Training Loss: 0.4403300881385803, Validation Loss: 0.4331859350204468\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 19: Training Loss: 0.43439080317815143, Validation Loss: 0.42743510007858276\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 20: Training Loss: 0.42923106253147125, Validation Loss: 0.42254340648651123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 21: Training Loss: 0.42460469901561737, Validation Loss: 0.4188407361507416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 22: Training Loss: 0.42101139823595685, Validation Loss: 0.4156903624534607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 23: Training Loss: 0.41834307710329693, Validation Loss: 0.41338977217674255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 24: Training Loss: 0.4165883461634318, Validation Loss: 0.412117600440979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 25: Training Loss: 0.41570129493872326, Validation Loss: 0.411590576171875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 26: Training Loss: 0.415120209256808, Validation Loss: 0.411253422498703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 27: Training Loss: 0.4149104356765747, Validation Loss: 0.41105136275291443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 28: Training Loss: 0.4148438523213069, Validation Loss: 0.41086965799331665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 29: Training Loss: 0.41480644543965656, Validation Loss: 0.41067683696746826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 30: Training Loss: 0.41464222967624664, Validation Loss: 0.4107130765914917\n",
      "Epoch 31: Training Loss: 0.4144519915183385, Validation Loss: 0.41032397747039795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 32: Training Loss: 0.4142704556385676, Validation Loss: 0.4099526107311249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 33: Training Loss: 0.4140509714682897, Validation Loss: 0.4096766412258148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 34: Training Loss: 0.4138609766960144, Validation Loss: 0.4094691574573517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 35: Training Loss: 0.4136721044778824, Validation Loss: 0.40956011414527893\n",
      "Epoch 36: Training Loss: 0.4135625461737315, Validation Loss: 0.4095323979854584\n",
      "Epoch 37: Training Loss: 0.41343526045481366, Validation Loss: 0.40920892357826233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 38: Training Loss: 0.41335564355055493, Validation Loss: 0.40901613235473633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 39: Training Loss: 0.41323094069957733, Validation Loss: 0.40908166766166687\n",
      "Epoch 40: Training Loss: 0.41311736901601154, Validation Loss: 0.4091928005218506\n",
      "Epoch 41: Training Loss: 0.4130036383867264, Validation Loss: 0.40908318758010864\n",
      "Epoch 42: Training Loss: 0.41285285850365955, Validation Loss: 0.40879565477371216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 43: Training Loss: 0.4127633919318517, Validation Loss: 0.40867990255355835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 44: Training Loss: 0.4126536051432292, Validation Loss: 0.4085831642150879\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 45: Training Loss: 0.412611057360967, Validation Loss: 0.40846920013427734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 46: Training Loss: 0.41247430940469104, Validation Loss: 0.40873250365257263\n",
      "Epoch 47: Training Loss: 0.41242457926273346, Validation Loss: 0.4084835648536682\n",
      "Epoch 48: Training Loss: 0.41225068767865497, Validation Loss: 0.40854060649871826\n",
      "Epoch 49: Training Loss: 0.4121621996164322, Validation Loss: 0.4084470570087433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 50: Training Loss: 0.4120793243249257, Validation Loss: 0.4082435965538025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 51: Training Loss: 0.41197745005289715, Validation Loss: 0.4082893133163452\n",
      "Epoch 52: Training Loss: 0.4118739316860835, Validation Loss: 0.4082798361778259\n",
      "Epoch 53: Training Loss: 0.4118175655603409, Validation Loss: 0.4080706536769867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 54: Training Loss: 0.41172875960667926, Validation Loss: 0.4079904854297638\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 55: Training Loss: 0.4116369883219401, Validation Loss: 0.4081854820251465\n",
      "Epoch 56: Training Loss: 0.41162391006946564, Validation Loss: 0.40829789638519287\n",
      "Epoch 57: Training Loss: 0.41143882771333057, Validation Loss: 0.4077952206134796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 58: Training Loss: 0.41135049362977344, Validation Loss: 0.4076038897037506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 59: Training Loss: 0.4113711466391881, Validation Loss: 0.4074991047382355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 60: Training Loss: 0.4113192856311798, Validation Loss: 0.4077834188938141\n",
      "Epoch 61: Training Loss: 0.4111545930306117, Validation Loss: 0.40760403871536255\n",
      "Epoch 62: Training Loss: 0.411039799451828, Validation Loss: 0.4076075851917267\n",
      "Epoch 63: Training Loss: 0.41096895933151245, Validation Loss: 0.4075707197189331\n",
      "Epoch 64: Training Loss: 0.41092248757680255, Validation Loss: 0.40749961137771606\n",
      "Epoch 65: Training Loss: 0.41086005171140033, Validation Loss: 0.4077955484390259\n",
      "Epoch 66: Training Loss: 0.41078416506449383, Validation Loss: 0.40760713815689087\n",
      "Epoch 67: Training Loss: 0.4107011407613754, Validation Loss: 0.407106876373291\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 68: Training Loss: 0.4106491208076477, Validation Loss: 0.4070937931537628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 69: Training Loss: 0.41074662407239276, Validation Loss: 0.40760448575019836\n",
      "Epoch 70: Training Loss: 0.410505289832751, Validation Loss: 0.4071912467479706\n",
      "Epoch 71: Training Loss: 0.4103974352280299, Validation Loss: 0.40689408779144287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 72: Training Loss: 0.4103344678878784, Validation Loss: 0.4069874882698059\n",
      "Epoch 73: Training Loss: 0.4102797160545985, Validation Loss: 0.40710726380348206\n",
      "Epoch 74: Training Loss: 0.41018887360890705, Validation Loss: 0.40692323446273804\n",
      "Epoch 75: Training Loss: 0.4102562169233958, Validation Loss: 0.4065972566604614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 76: Training Loss: 0.4101217985153198, Validation Loss: 0.40704596042633057\n",
      "Epoch 77: Training Loss: 0.41002313792705536, Validation Loss: 0.40707871317863464\n",
      "Epoch 78: Training Loss: 0.40992682178815204, Validation Loss: 0.40663671493530273\n",
      "Epoch 79: Training Loss: 0.40985850989818573, Validation Loss: 0.40644314885139465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 80: Training Loss: 0.40982600549856824, Validation Loss: 0.4064142405986786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 81: Training Loss: 0.40975675483544666, Validation Loss: 0.4067578613758087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: Training Loss: 0.409707248210907, Validation Loss: 0.4067399203777313\n",
      "Epoch 83: Training Loss: 0.40961433947086334, Validation Loss: 0.4062787890434265\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 84: Training Loss: 0.40956981976826984, Validation Loss: 0.4062157869338989\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 85: Training Loss: 0.409499595562617, Validation Loss: 0.40621811151504517\n",
      "Epoch 86: Training Loss: 0.4094204654296239, Validation Loss: 0.40643733739852905\n",
      "Epoch 87: Training Loss: 0.4094889511664708, Validation Loss: 0.4064721465110779\n",
      "Epoch 88: Training Loss: 0.40929973125457764, Validation Loss: 0.40594688057899475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 89: Training Loss: 0.40931279957294464, Validation Loss: 0.40571120381355286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 90: Training Loss: 0.40925464034080505, Validation Loss: 0.40592679381370544\n",
      "Epoch 91: Training Loss: 0.4091101934512456, Validation Loss: 0.4061220586299896\n",
      "Epoch 92: Training Loss: 0.4091317454973857, Validation Loss: 0.4062897562980652\n",
      "Epoch 93: Training Loss: 0.40907569229602814, Validation Loss: 0.40568429231643677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 94: Training Loss: 0.4089979330698649, Validation Loss: 0.40552234649658203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 95: Training Loss: 0.4089260846376419, Validation Loss: 0.4057406783103943\n",
      "Epoch 96: Training Loss: 0.40887219707171124, Validation Loss: 0.4056377708911896\n",
      "Epoch 97: Training Loss: 0.4088251789410909, Validation Loss: 0.40584906935691833\n",
      "Epoch 98: Training Loss: 0.4087998221317927, Validation Loss: 0.40564972162246704\n",
      "Epoch 99: Training Loss: 0.40871574481328327, Validation Loss: 0.40516531467437744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 100: Training Loss: 0.4087269405523936, Validation Loss: 0.4054621160030365\n",
      "Epoch 101: Training Loss: 0.4085971266031265, Validation Loss: 0.40541863441467285\n",
      "Epoch 102: Training Loss: 0.4086030026276906, Validation Loss: 0.405241459608078\n",
      "Epoch 103: Training Loss: 0.40850136677424115, Validation Loss: 0.40552034974098206\n",
      "Epoch 104: Training Loss: 0.4084496448437373, Validation Loss: 0.4053376317024231\n",
      "Epoch 105: Training Loss: 0.4085264454285304, Validation Loss: 0.40469974279403687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 106: Training Loss: 0.40845684210459393, Validation Loss: 0.40509819984436035\n",
      "Epoch 107: Training Loss: 0.40831606090068817, Validation Loss: 0.40538910031318665\n",
      "Epoch 108: Training Loss: 0.40822698672612506, Validation Loss: 0.4049617648124695\n",
      "Epoch 109: Training Loss: 0.4082520604133606, Validation Loss: 0.40463683009147644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 110: Training Loss: 0.4081711421410243, Validation Loss: 0.40501585602760315\n",
      "Epoch 111: Training Loss: 0.40827525158723194, Validation Loss: 0.4056728780269623\n",
      "Epoch 112: Training Loss: 0.4081424723068873, Validation Loss: 0.4049600064754486\n",
      "Epoch 113: Training Loss: 0.40793829162915546, Validation Loss: 0.4044249951839447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 114: Training Loss: 0.4080118040243785, Validation Loss: 0.4042203426361084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 115: Training Loss: 0.4079507937033971, Validation Loss: 0.40460532903671265\n",
      "Epoch 116: Training Loss: 0.40782084067662555, Validation Loss: 0.4048806130886078\n",
      "Epoch 117: Training Loss: 0.4078217993179957, Validation Loss: 0.40490683913230896\n",
      "Epoch 118: Training Loss: 0.40801040331522626, Validation Loss: 0.4053269028663635\n",
      "Epoch 119: Training Loss: 0.4077608933051427, Validation Loss: 0.4042959213256836\n",
      "Epoch 120: Training Loss: 0.40775857369105023, Validation Loss: 0.40387633442878723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 121: Training Loss: 0.4076975037654241, Validation Loss: 0.40440183877944946\n",
      "Epoch 122: Training Loss: 0.40785114963849384, Validation Loss: 0.4051227569580078\n",
      "Epoch 123: Training Loss: 0.40771204729874927, Validation Loss: 0.4041304886341095\n",
      "Epoch 124: Training Loss: 0.4074804385503133, Validation Loss: 0.4040485620498657\n",
      "Epoch 125: Training Loss: 0.4074573963880539, Validation Loss: 0.4041880667209625\n",
      "Epoch 126: Training Loss: 0.4074116051197052, Validation Loss: 0.4041849672794342\n",
      "Epoch 127: Training Loss: 0.40741204222043353, Validation Loss: 0.40390723943710327\n",
      "Epoch 128: Training Loss: 0.40731963018576306, Validation Loss: 0.40418511629104614\n",
      "Epoch 129: Training Loss: 0.40729201833407086, Validation Loss: 0.40407347679138184\n",
      "Epoch 130: Training Loss: 0.407272145152092, Validation Loss: 0.4039750099182129\n",
      "Epoch 131: Training Loss: 0.40719706813494366, Validation Loss: 0.40383777022361755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 132: Training Loss: 0.4071543614069621, Validation Loss: 0.40370431542396545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 133: Training Loss: 0.4071367333332698, Validation Loss: 0.4035801291465759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 134: Training Loss: 0.40710437794526416, Validation Loss: 0.4038296937942505\n",
      "Epoch 135: Training Loss: 0.4070335477590561, Validation Loss: 0.40380945801734924\n",
      "Epoch 136: Training Loss: 0.40699925025304157, Validation Loss: 0.4037337899208069\n",
      "Epoch 137: Training Loss: 0.4071188122034073, Validation Loss: 0.40403056144714355\n",
      "Epoch 138: Training Loss: 0.40691039462884265, Validation Loss: 0.40340739488601685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 139: Training Loss: 0.4068742295106252, Validation Loss: 0.40314486622810364\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 140: Training Loss: 0.40689461429913837, Validation Loss: 0.40346455574035645\n",
      "Epoch 141: Training Loss: 0.40699686110019684, Validation Loss: 0.40384000539779663\n",
      "Epoch 142: Training Loss: 0.4067407697439194, Validation Loss: 0.40319690108299255\n",
      "Epoch 143: Training Loss: 0.4068093001842499, Validation Loss: 0.4029187560081482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 144: Training Loss: 0.40669772525628406, Validation Loss: 0.4034020006656647\n",
      "Epoch 145: Training Loss: 0.40668374796708423, Validation Loss: 0.40379300713539124\n",
      "Epoch 146: Training Loss: 0.4067075749238332, Validation Loss: 0.40345409512519836\n",
      "Epoch 147: Training Loss: 0.4068157722552617, Validation Loss: 0.40260961651802063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 148: Training Loss: 0.4067019373178482, Validation Loss: 0.40280094742774963\n",
      "Epoch 149: Training Loss: 0.40649690727392834, Validation Loss: 0.40358614921569824\n",
      "Epoch 150: Training Loss: 0.40656041105588275, Validation Loss: 0.40377724170684814\n",
      "Epoch 151: Training Loss: 0.4064912547667821, Validation Loss: 0.4030865728855133\n",
      "Epoch 152: Training Loss: 0.4064169277747472, Validation Loss: 0.40247946977615356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 153: Training Loss: 0.4064654658238093, Validation Loss: 0.40291380882263184\n",
      "Epoch 154: Training Loss: 0.4063299546639125, Validation Loss: 0.40332749485969543\n",
      "Epoch 155: Training Loss: 0.40631963312625885, Validation Loss: 0.4029795229434967\n",
      "Epoch 156: Training Loss: 0.4062490612268448, Validation Loss: 0.4028346538543701\n",
      "Epoch 157: Training Loss: 0.406250258286794, Validation Loss: 0.4026345908641815\n",
      "Epoch 158: Training Loss: 0.40620217223962146, Validation Loss: 0.40292999148368835\n",
      "Epoch 159: Training Loss: 0.4062108099460602, Validation Loss: 0.40272653102874756\n",
      "Epoch 160: Training Loss: 0.40616826713085175, Validation Loss: 0.40294909477233887\n",
      "Epoch 161: Training Loss: 0.40612636506557465, Validation Loss: 0.40258660912513733\n",
      "Epoch 162: Training Loss: 0.4060547600189845, Validation Loss: 0.4025353789329529\n",
      "Epoch 163: Training Loss: 0.4060276399056117, Validation Loss: 0.4026735723018646\n",
      "Epoch 164: Training Loss: 0.4060235371192296, Validation Loss: 0.4024357497692108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 165: Training Loss: 0.40596775710582733, Validation Loss: 0.40270116925239563\n",
      "Epoch 166: Training Loss: 0.4060088098049164, Validation Loss: 0.40240395069122314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 167: Training Loss: 0.40592702229817706, Validation Loss: 0.402472585439682\n",
      "Epoch 168: Training Loss: 0.4058944185574849, Validation Loss: 0.40277189016342163\n",
      "Epoch 169: Training Loss: 0.40586449205875397, Validation Loss: 0.4023574888706207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 170: Training Loss: 0.4058266580104828, Validation Loss: 0.4020916223526001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 171: Training Loss: 0.40579699476559955, Validation Loss: 0.4021866023540497\n",
      "Epoch 172: Training Loss: 0.4057660500208537, Validation Loss: 0.40225160121917725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173: Training Loss: 0.40570972363154095, Validation Loss: 0.4025936722755432\n",
      "Epoch 174: Training Loss: 0.405824954311053, Validation Loss: 0.4021640717983246\n",
      "Epoch 175: Training Loss: 0.40588706235090893, Validation Loss: 0.40297824144363403\n",
      "Epoch 176: Training Loss: 0.4056837360064189, Validation Loss: 0.40211012959480286\n",
      "Epoch 177: Training Loss: 0.40591153005758923, Validation Loss: 0.4015367031097412\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 178: Training Loss: 0.4056403984626134, Validation Loss: 0.40224796533584595\n",
      "Epoch 179: Training Loss: 0.4058287938435872, Validation Loss: 0.4038219153881073\n",
      "Epoch 180: Training Loss: 0.4058077385028203, Validation Loss: 0.40235522389411926\n",
      "Epoch 181: Training Loss: 0.4054326266050339, Validation Loss: 0.4013524651527405\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 182: Training Loss: 0.40569856266180676, Validation Loss: 0.40151235461235046\n",
      "Epoch 183: Training Loss: 0.40566399693489075, Validation Loss: 0.40270909667015076\n",
      "Epoch 184: Training Loss: 0.4055069337288539, Validation Loss: 0.40207216143608093\n",
      "Epoch 185: Training Loss: 0.40551071365674335, Validation Loss: 0.4014834761619568\n",
      "Epoch 186: Training Loss: 0.40554170807202655, Validation Loss: 0.40221381187438965\n",
      "Epoch 187: Training Loss: 0.40536464750766754, Validation Loss: 0.4018554389476776\n",
      "Epoch 188: Training Loss: 0.40535056591033936, Validation Loss: 0.40167236328125\n",
      "Epoch 189: Training Loss: 0.4053732802470525, Validation Loss: 0.4014013111591339\n",
      "Epoch 190: Training Loss: 0.40552539626757306, Validation Loss: 0.4025733768939972\n",
      "Epoch 191: Training Loss: 0.40536566575368244, Validation Loss: 0.4016992449760437\n",
      "Epoch 192: Training Loss: 0.4052782555421193, Validation Loss: 0.40147003531455994\n",
      "Epoch 193: Training Loss: 0.4052590032418569, Validation Loss: 0.4018261134624481\n",
      "Epoch 194: Training Loss: 0.4052243580420812, Validation Loss: 0.40181395411491394\n",
      "Epoch 195: Training Loss: 0.4052303036053975, Validation Loss: 0.40122488141059875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 196: Training Loss: 0.4051656822363536, Validation Loss: 0.4014654755592346\n",
      "Epoch 197: Training Loss: 0.40510376791159314, Validation Loss: 0.4017069339752197\n",
      "Epoch 198: Training Loss: 0.4050756245851517, Validation Loss: 0.402240127325058\n",
      "Epoch 199: Training Loss: 0.4052451401948929, Validation Loss: 0.4020587205886841\n",
      "Epoch 200: Training Loss: 0.40503881871700287, Validation Loss: 0.4011118412017822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 201: Training Loss: 0.4050866315762202, Validation Loss: 0.4009416103363037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 202: Training Loss: 0.40501195192337036, Validation Loss: 0.4015476107597351\n",
      "Epoch 203: Training Loss: 0.4049581487973531, Validation Loss: 0.40193799138069153\n",
      "Epoch 204: Training Loss: 0.4049866795539856, Validation Loss: 0.40151792764663696\n",
      "Epoch 205: Training Loss: 0.40489503741264343, Validation Loss: 0.4010019302368164\n",
      "Epoch 206: Training Loss: 0.4049409677584966, Validation Loss: 0.40098878741264343\n",
      "Epoch 207: Training Loss: 0.4049394726753235, Validation Loss: 0.4015538990497589\n",
      "Epoch 208: Training Loss: 0.4048658460378647, Validation Loss: 0.4013230800628662\n",
      "Epoch 209: Training Loss: 0.4048321147759755, Validation Loss: 0.401197612285614\n",
      "Epoch 210: Training Loss: 0.40483953555425006, Validation Loss: 0.4012082517147064\n",
      "Epoch 211: Training Loss: 0.4047771046559016, Validation Loss: 0.4010835587978363\n",
      "Epoch 212: Training Loss: 0.40480323632558185, Validation Loss: 0.4010128974914551\n",
      "Epoch 213: Training Loss: 0.40496181945006055, Validation Loss: 0.40151309967041016\n",
      "Epoch 214: Training Loss: 0.40476831297079724, Validation Loss: 0.4008038341999054\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 215: Training Loss: 0.4047706574201584, Validation Loss: 0.400799423456192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 216: Training Loss: 0.40466711421807605, Validation Loss: 0.40145596861839294\n",
      "Epoch 217: Training Loss: 0.40469130376974743, Validation Loss: 0.4014710485935211\n",
      "Epoch 218: Training Loss: 0.40463536977767944, Validation Loss: 0.4009000062942505\n",
      "Epoch 219: Training Loss: 0.4046875188748042, Validation Loss: 0.4006621539592743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 220: Training Loss: 0.40479585031668347, Validation Loss: 0.4010502099990845\n",
      "Epoch 221: Training Loss: 0.404570331176122, Validation Loss: 0.40078115463256836\n",
      "Epoch 222: Training Loss: 0.4045763860146205, Validation Loss: 0.4006521999835968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 223: Training Loss: 0.4045787702004115, Validation Loss: 0.40075576305389404\n",
      "Epoch 224: Training Loss: 0.4047343482573827, Validation Loss: 0.4018869698047638\n",
      "Epoch 225: Training Loss: 0.4045921266078949, Validation Loss: 0.4009625017642975\n",
      "Epoch 226: Training Loss: 0.4045347621043523, Validation Loss: 0.4004250764846802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 227: Training Loss: 0.4045639286438624, Validation Loss: 0.40061360597610474\n",
      "Epoch 228: Training Loss: 0.4044987956682841, Validation Loss: 0.40157344937324524\n",
      "Epoch 229: Training Loss: 0.40456819037596387, Validation Loss: 0.4010295569896698\n",
      "Epoch 230: Training Loss: 0.4043818960587184, Validation Loss: 0.4004416763782501\n",
      "Epoch 231: Training Loss: 0.40447578330834705, Validation Loss: 0.40070655941963196\n",
      "Epoch 232: Training Loss: 0.404390091697375, Validation Loss: 0.40074995160102844\n",
      "Epoch 233: Training Loss: 0.40438058972358704, Validation Loss: 0.4005393385887146\n",
      "Epoch 234: Training Loss: 0.4043531268835068, Validation Loss: 0.4005984663963318\n",
      "Epoch 235: Training Loss: 0.4043128987153371, Validation Loss: 0.4007778763771057\n",
      "Epoch 236: Training Loss: 0.40429937342802685, Validation Loss: 0.4007152020931244\n",
      "Epoch 237: Training Loss: 0.40426310896873474, Validation Loss: 0.4005374014377594\n",
      "Epoch 238: Training Loss: 0.40435391664505005, Validation Loss: 0.4005376994609833\n",
      "Epoch 239: Training Loss: 0.404249832034111, Validation Loss: 0.4002639353275299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 240: Training Loss: 0.4042673508326213, Validation Loss: 0.4006931781768799\n",
      "Epoch 241: Training Loss: 0.40422412753105164, Validation Loss: 0.4005816578865051\n",
      "Epoch 242: Training Loss: 0.40420342485109967, Validation Loss: 0.4005323350429535\n",
      "Epoch 243: Training Loss: 0.40417525668938953, Validation Loss: 0.40032899379730225\n",
      "Epoch 244: Training Loss: 0.4041772534449895, Validation Loss: 0.4005092978477478\n",
      "Epoch 245: Training Loss: 0.40413297712802887, Validation Loss: 0.400488942861557\n",
      "Epoch 246: Training Loss: 0.40413111944993335, Validation Loss: 0.40050408244132996\n",
      "Epoch 247: Training Loss: 0.4041980604330699, Validation Loss: 0.40030357241630554\n",
      "Epoch 248: Training Loss: 0.40423910816510517, Validation Loss: 0.4007617235183716\n",
      "Epoch 249: Training Loss: 0.4043048520882924, Validation Loss: 0.39991599321365356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 250: Training Loss: 0.4041171173254649, Validation Loss: 0.4003617763519287\n",
      "Epoch 251: Training Loss: 0.4044789622227351, Validation Loss: 0.4019317924976349\n",
      "Epoch 252: Training Loss: 0.4042672763268153, Validation Loss: 0.40048885345458984\n",
      "Epoch 253: Training Loss: 0.4043366312980652, Validation Loss: 0.3995981514453888\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 254: Training Loss: 0.40420180559158325, Validation Loss: 0.40012574195861816\n",
      "Epoch 255: Training Loss: 0.40400656561056775, Validation Loss: 0.40106114745140076\n",
      "Epoch 256: Training Loss: 0.4040459543466568, Validation Loss: 0.40054070949554443\n",
      "Epoch 257: Training Loss: 0.4039415667454402, Validation Loss: 0.399819016456604\n",
      "Epoch 258: Training Loss: 0.4040095309416453, Validation Loss: 0.39980262517929077\n",
      "Epoch 259: Training Loss: 0.4040498485167821, Validation Loss: 0.4002661108970642\n",
      "Epoch 260: Training Loss: 0.40388425687948865, Validation Loss: 0.4001569151878357\n",
      "Epoch 261: Training Loss: 0.40400023261706036, Validation Loss: 0.3998624086380005\n",
      "Epoch 262: Training Loss: 0.40387675662835437, Validation Loss: 0.4001942574977875\n",
      "Epoch 263: Training Loss: 0.40384019414583844, Validation Loss: 0.400563508272171\n",
      "Epoch 264: Training Loss: 0.40392330785592395, Validation Loss: 0.3999767303466797\n",
      "Epoch 265: Training Loss: 0.40381001432736713, Validation Loss: 0.4001252055168152\n",
      "Epoch 266: Training Loss: 0.4038552890221278, Validation Loss: 0.4003528952598572\n",
      "Epoch 267: Training Loss: 0.4037665178378423, Validation Loss: 0.39991387724876404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268: Training Loss: 0.4038509279489517, Validation Loss: 0.39976176619529724\n",
      "Epoch 269: Training Loss: 0.4037911246220271, Validation Loss: 0.40034976601600647\n",
      "Epoch 270: Training Loss: 0.4038195957740148, Validation Loss: 0.3999960720539093\n",
      "Epoch 271: Training Loss: 0.4037083188692729, Validation Loss: 0.3999289274215698\n",
      "Epoch 272: Training Loss: 0.40377721190452576, Validation Loss: 0.40031954646110535\n",
      "Epoch 273: Training Loss: 0.4036935170491536, Validation Loss: 0.39967551827430725\n",
      "Epoch 274: Training Loss: 0.4039343496163686, Validation Loss: 0.3995337188243866\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 275: Training Loss: 0.4037018169959386, Validation Loss: 0.4007335305213928\n",
      "Epoch 276: Training Loss: 0.4037960668404897, Validation Loss: 0.4002841114997864\n",
      "Epoch 277: Training Loss: 0.40362637241681415, Validation Loss: 0.399415522813797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 278: Training Loss: 0.4037717630465825, Validation Loss: 0.3994765281677246\n",
      "Epoch 279: Training Loss: 0.4038242946068446, Validation Loss: 0.4004257917404175\n",
      "Epoch 280: Training Loss: 0.4036307285229365, Validation Loss: 0.3997931480407715\n",
      "Epoch 281: Training Loss: 0.4035900682210922, Validation Loss: 0.3994341790676117\n",
      "Epoch 282: Training Loss: 0.4035734385251999, Validation Loss: 0.4000178575515747\n",
      "Epoch 283: Training Loss: 0.403560146689415, Validation Loss: 0.40014204382896423\n",
      "Epoch 284: Training Loss: 0.40358992914358777, Validation Loss: 0.3994932174682617\n",
      "Epoch 285: Training Loss: 0.40362711250782013, Validation Loss: 0.3994227945804596\n",
      "Epoch 286: Training Loss: 0.403564915060997, Validation Loss: 0.4004286229610443\n",
      "Epoch 287: Training Loss: 0.40350397924582165, Validation Loss: 0.3999684751033783\n",
      "Epoch 288: Training Loss: 0.4037864108880361, Validation Loss: 0.3990044891834259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 289: Training Loss: 0.4036593735218048, Validation Loss: 0.39973798394203186\n",
      "Epoch 290: Training Loss: 0.403378297885259, Validation Loss: 0.4003715217113495\n",
      "Epoch 291: Training Loss: 0.4034661799669266, Validation Loss: 0.3999377191066742\n",
      "Epoch 292: Training Loss: 0.40339915951093036, Validation Loss: 0.3992638885974884\n",
      "Epoch 293: Training Loss: 0.40363748371601105, Validation Loss: 0.399728000164032\n",
      "Epoch 294: Training Loss: 0.4033782184123993, Validation Loss: 0.3995426893234253\n",
      "Epoch 295: Training Loss: 0.40338754157225293, Validation Loss: 0.39989790320396423\n",
      "Epoch 296: Training Loss: 0.40335258344809216, Validation Loss: 0.39952096343040466\n",
      "Epoch 297: Training Loss: 0.4033070107301076, Validation Loss: 0.39943763613700867\n",
      "Epoch 298: Training Loss: 0.40339672565460205, Validation Loss: 0.39919188618659973\n",
      "Epoch 299: Training Loss: 0.40349143246809643, Validation Loss: 0.4003230333328247\n",
      "Epoch 300: Training Loss: 0.40331989030043286, Validation Loss: 0.3997293710708618\n",
      "Epoch 301: Training Loss: 0.4032387286424637, Validation Loss: 0.3993183672428131\n",
      "Epoch 302: Training Loss: 0.40335674583911896, Validation Loss: 0.39907312393188477\n",
      "Epoch 303: Training Loss: 0.4032447338104248, Validation Loss: 0.39988094568252563\n",
      "Epoch 304: Training Loss: 0.40336547295252484, Validation Loss: 0.40052929520606995\n",
      "Epoch 305: Training Loss: 0.40333202481269836, Validation Loss: 0.39977943897247314\n",
      "Epoch 306: Training Loss: 0.40329991777737934, Validation Loss: 0.39886584877967834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 307: Training Loss: 0.4032544444004695, Validation Loss: 0.39948800206184387\n",
      "Epoch 308: Training Loss: 0.40330750743548077, Validation Loss: 0.40063852071762085\n",
      "Epoch 309: Training Loss: 0.4035624812046687, Validation Loss: 0.4003661870956421\n",
      "Epoch 310: Training Loss: 0.40323986609776813, Validation Loss: 0.3987916111946106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 311: Training Loss: 0.4033720791339874, Validation Loss: 0.3989465534687042\n",
      "Epoch 312: Training Loss: 0.403373325864474, Validation Loss: 0.4000924229621887\n",
      "Epoch 313: Training Loss: 0.403126060962677, Validation Loss: 0.39952805638313293\n",
      "Epoch 314: Training Loss: 0.4030720740556717, Validation Loss: 0.3991328179836273\n",
      "Epoch 315: Training Loss: 0.4032297531763713, Validation Loss: 0.3989250361919403\n",
      "Epoch 316: Training Loss: 0.4030781189600627, Validation Loss: 0.39974892139434814\n",
      "Epoch 317: Training Loss: 0.40306011339028675, Validation Loss: 0.39983662962913513\n",
      "Epoch 318: Training Loss: 0.40304261445999146, Validation Loss: 0.3996448814868927\n",
      "Epoch 319: Training Loss: 0.4029674728711446, Validation Loss: 0.3990131914615631\n",
      "Epoch 320: Training Loss: 0.4029990533987681, Validation Loss: 0.39927059412002563\n",
      "Epoch 321: Training Loss: 0.40297206739584607, Validation Loss: 0.3996829688549042\n",
      "Epoch 322: Training Loss: 0.4029572159051895, Validation Loss: 0.39932262897491455\n",
      "Epoch 323: Training Loss: 0.4030157874027888, Validation Loss: 0.39900490641593933\n",
      "Epoch 324: Training Loss: 0.40297140677769977, Validation Loss: 0.39977771043777466\n",
      "Epoch 325: Training Loss: 0.4029685507218043, Validation Loss: 0.39932191371917725\n",
      "Epoch 326: Training Loss: 0.40314409633477527, Validation Loss: 0.39863646030426025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 327: Training Loss: 0.4029947519302368, Validation Loss: 0.3995056748390198\n",
      "Epoch 328: Training Loss: 0.4028843243916829, Validation Loss: 0.39983218908309937\n",
      "Epoch 329: Training Loss: 0.4029454042514165, Validation Loss: 0.39927056431770325\n",
      "Epoch 330: Training Loss: 0.40283149977525073, Validation Loss: 0.39914214611053467\n",
      "Epoch 331: Training Loss: 0.402786448597908, Validation Loss: 0.39945748448371887\n",
      "Epoch 332: Training Loss: 0.40288786093393963, Validation Loss: 0.3995911777019501\n",
      "Epoch 333: Training Loss: 0.4029475698868434, Validation Loss: 0.3986411392688751\n",
      "Epoch 334: Training Loss: 0.4030188322067261, Validation Loss: 0.3991377651691437\n",
      "Epoch 335: Training Loss: 0.4027535269657771, Validation Loss: 0.3989694118499756\n",
      "Epoch 336: Training Loss: 0.4027455945809682, Validation Loss: 0.3991118371486664\n",
      "Epoch 337: Training Loss: 0.4027478098869324, Validation Loss: 0.39933696389198303\n",
      "Epoch 338: Training Loss: 0.4027101397514343, Validation Loss: 0.39945894479751587\n",
      "Epoch 339: Training Loss: 0.40274931987126666, Validation Loss: 0.39916354417800903\n",
      "Epoch 340: Training Loss: 0.40271587669849396, Validation Loss: 0.3990207016468048\n",
      "Epoch 341: Training Loss: 0.4026530434687932, Validation Loss: 0.3992258906364441\n",
      "Epoch 342: Training Loss: 0.40270038942495984, Validation Loss: 0.3993181586265564\n",
      "Epoch 343: Training Loss: 0.40282224615414935, Validation Loss: 0.3987346589565277\n",
      "Epoch 344: Training Loss: 0.40259772539138794, Validation Loss: 0.39930635690689087\n",
      "Epoch 345: Training Loss: 0.4026850362618764, Validation Loss: 0.39987319707870483\n",
      "Epoch 346: Training Loss: 0.40262943009535473, Validation Loss: 0.39897918701171875\n",
      "Epoch 347: Training Loss: 0.4025692045688629, Validation Loss: 0.39872628450393677\n",
      "Epoch 348: Training Loss: 0.4026491542657216, Validation Loss: 0.39880064129829407\n",
      "Epoch 349: Training Loss: 0.40254247188568115, Validation Loss: 0.3995516002178192\n",
      "Epoch 350: Training Loss: 0.4025900314251582, Validation Loss: 0.39915552735328674\n",
      "Epoch 351: Training Loss: 0.4026479572057724, Validation Loss: 0.3986116051673889\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 352: Training Loss: 0.40253029266993207, Validation Loss: 0.39918896555900574\n",
      "Epoch 353: Training Loss: 0.40248873829841614, Validation Loss: 0.399168461561203\n",
      "Epoch 354: Training Loss: 0.4025305559237798, Validation Loss: 0.3992784917354584\n",
      "Epoch 355: Training Loss: 0.40257448454697925, Validation Loss: 0.3987167179584503\n",
      "Epoch 356: Training Loss: 0.4025234083334605, Validation Loss: 0.3992387354373932\n",
      "Epoch 357: Training Loss: 0.4024729033311208, Validation Loss: 0.39875146746635437\n",
      "Epoch 358: Training Loss: 0.4024372200171153, Validation Loss: 0.3988784849643707\n",
      "Epoch 359: Training Loss: 0.4024459471305211, Validation Loss: 0.3995508849620819\n",
      "Epoch 360: Training Loss: 0.40249116718769073, Validation Loss: 0.3992081880569458\n",
      "Epoch 361: Training Loss: 0.40251607696215314, Validation Loss: 0.39826706051826477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 362: Training Loss: 0.4024937003850937, Validation Loss: 0.39885658025741577\n",
      "Epoch 363: Training Loss: 0.40232514838377637, Validation Loss: 0.39984947443008423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364: Training Loss: 0.4024309118588765, Validation Loss: 0.39904505014419556\n",
      "Epoch 365: Training Loss: 0.40260955194632214, Validation Loss: 0.39840054512023926\n",
      "Epoch 366: Training Loss: 0.4024038414160411, Validation Loss: 0.3993177115917206\n",
      "Epoch 367: Training Loss: 0.40231655538082123, Validation Loss: 0.39910414814949036\n",
      "Epoch 368: Training Loss: 0.40228677292664844, Validation Loss: 0.39883437752723694\n",
      "Epoch 369: Training Loss: 0.402343546350797, Validation Loss: 0.39868009090423584\n",
      "Epoch 370: Training Loss: 0.40225480993588764, Validation Loss: 0.3989422023296356\n",
      "Epoch 371: Training Loss: 0.4022267311811447, Validation Loss: 0.39947158098220825\n",
      "Epoch 372: Training Loss: 0.4022728403409322, Validation Loss: 0.39896297454833984\n",
      "Epoch 373: Training Loss: 0.4022579143444697, Validation Loss: 0.39882510900497437\n",
      "Epoch 374: Training Loss: 0.4022764017184575, Validation Loss: 0.3983553349971771\n",
      "Epoch 375: Training Loss: 0.4024117986361186, Validation Loss: 0.3990776836872101\n",
      "Epoch 376: Training Loss: 0.4021661728620529, Validation Loss: 0.3986193835735321\n",
      "Epoch 377: Training Loss: 0.4021442135175069, Validation Loss: 0.3986448049545288\n",
      "Epoch 378: Training Loss: 0.4021759281555812, Validation Loss: 0.39901262521743774\n",
      "Epoch 379: Training Loss: 0.4021288404862086, Validation Loss: 0.39851054549217224\n",
      "Epoch 380: Training Loss: 0.40211273233095807, Validation Loss: 0.398774117231369\n",
      "Epoch 381: Training Loss: 0.4021080980698268, Validation Loss: 0.3989017605781555\n",
      "Epoch 382: Training Loss: 0.4021008213361104, Validation Loss: 0.3986607491970062\n",
      "Epoch 383: Training Loss: 0.40211443106333417, Validation Loss: 0.399088054895401\n",
      "Epoch 384: Training Loss: 0.40224942564964294, Validation Loss: 0.39824995398521423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 385: Training Loss: 0.4021249860525131, Validation Loss: 0.3989267349243164\n",
      "Epoch 386: Training Loss: 0.402060866355896, Validation Loss: 0.3989453613758087\n",
      "Epoch 387: Training Loss: 0.4019997666279475, Validation Loss: 0.39871591329574585\n",
      "Epoch 388: Training Loss: 0.40212548772494, Validation Loss: 0.39833295345306396\n",
      "Epoch 389: Training Loss: 0.4019663681586583, Validation Loss: 0.3990483283996582\n",
      "Epoch 390: Training Loss: 0.40199586749076843, Validation Loss: 0.3992961347103119\n",
      "Epoch 391: Training Loss: 0.40197014808654785, Validation Loss: 0.3986615836620331\n",
      "Epoch 392: Training Loss: 0.4019082685311635, Validation Loss: 0.3984229564666748\n",
      "Epoch 393: Training Loss: 0.4020502169926961, Validation Loss: 0.39906415343284607\n",
      "Epoch 394: Training Loss: 0.40199512739976245, Validation Loss: 0.3988420069217682\n",
      "Epoch 395: Training Loss: 0.40199921528498334, Validation Loss: 0.39806675910949707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 396: Training Loss: 0.40199021498362225, Validation Loss: 0.3988426625728607\n",
      "Epoch 397: Training Loss: 0.40183547139167786, Validation Loss: 0.3990207314491272\n",
      "Epoch 398: Training Loss: 0.40192130704720813, Validation Loss: 0.39886370301246643\n",
      "Epoch 399: Training Loss: 0.4019377678632736, Validation Loss: 0.3990001976490021\n",
      "Epoch 400: Training Loss: 0.40178801119327545, Validation Loss: 0.39840051531791687\n",
      "Epoch 401: Training Loss: 0.4018911272287369, Validation Loss: 0.39836058020591736\n",
      "Epoch 402: Training Loss: 0.4017622421185176, Validation Loss: 0.39899587631225586\n",
      "Epoch 403: Training Loss: 0.4020386089881261, Validation Loss: 0.3999740481376648\n",
      "Epoch 404: Training Loss: 0.4018673300743103, Validation Loss: 0.3984946012496948\n",
      "Epoch 405: Training Loss: 0.4017130533854167, Validation Loss: 0.3980353772640228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 406: Training Loss: 0.4019738833109538, Validation Loss: 0.39823800325393677\n",
      "Epoch 407: Training Loss: 0.40166928867499035, Validation Loss: 0.39959636330604553\n",
      "Epoch 408: Training Loss: 0.40188149114449817, Validation Loss: 0.3994036614894867\n",
      "Epoch 409: Training Loss: 0.40172237654527027, Validation Loss: 0.39812159538269043\n",
      "Epoch 410: Training Loss: 0.4020693401495616, Validation Loss: 0.3978654146194458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 411: Training Loss: 0.40184210737546283, Validation Loss: 0.39942190051078796\n",
      "Epoch 412: Training Loss: 0.4018539786338806, Validation Loss: 0.3992670774459839\n",
      "Epoch 413: Training Loss: 0.4016668399175008, Validation Loss: 0.3980640172958374\n",
      "Epoch 414: Training Loss: 0.4019460181395213, Validation Loss: 0.39785638451576233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 415: Training Loss: 0.40186071395874023, Validation Loss: 0.3994240462779999\n",
      "Epoch 416: Training Loss: 0.4018351783355077, Validation Loss: 0.3994271755218506\n",
      "Epoch 417: Training Loss: 0.4016941984494527, Validation Loss: 0.3983808755874634\n",
      "Epoch 418: Training Loss: 0.4015861451625824, Validation Loss: 0.3982945680618286\n",
      "Epoch 419: Training Loss: 0.40157370766003925, Validation Loss: 0.3983880281448364\n",
      "Epoch 420: Training Loss: 0.4016287873188655, Validation Loss: 0.39875441789627075\n",
      "Epoch 421: Training Loss: 0.40166263779004413, Validation Loss: 0.3982493579387665\n",
      "Epoch 422: Training Loss: 0.4015322724978129, Validation Loss: 0.39885056018829346\n",
      "Epoch 423: Training Loss: 0.4018251101175944, Validation Loss: 0.39925721287727356\n",
      "Epoch 424: Training Loss: 0.40146345893541974, Validation Loss: 0.3980381190776825\n",
      "Epoch 425: Training Loss: 0.40156545241673786, Validation Loss: 0.3979153633117676\n",
      "Epoch 426: Training Loss: 0.4015803982814153, Validation Loss: 0.3986041247844696\n",
      "Epoch 427: Training Loss: 0.4014660169680913, Validation Loss: 0.3984725773334503\n",
      "Epoch 428: Training Loss: 0.40145187576611835, Validation Loss: 0.3984109163284302\n",
      "Epoch 429: Training Loss: 0.40145303805669147, Validation Loss: 0.39874184131622314\n",
      "Epoch 430: Training Loss: 0.40141476194063824, Validation Loss: 0.3983615040779114\n",
      "Epoch 431: Training Loss: 0.4014537235101064, Validation Loss: 0.3980426490306854\n",
      "Epoch 432: Training Loss: 0.40175044039885205, Validation Loss: 0.3989328444004059\n",
      "Epoch 433: Training Loss: 0.4013924052317937, Validation Loss: 0.39824092388153076\n",
      "Epoch 434: Training Loss: 0.4013938556114833, Validation Loss: 0.3982659578323364\n",
      "Epoch 435: Training Loss: 0.40133530894915265, Validation Loss: 0.3984038233757019\n",
      "Epoch 436: Training Loss: 0.40133288502693176, Validation Loss: 0.3985288143157959\n",
      "Epoch 437: Training Loss: 0.40147053201993305, Validation Loss: 0.3982943296432495\n",
      "Epoch 438: Training Loss: 0.4013640582561493, Validation Loss: 0.39907270669937134\n",
      "Epoch 439: Training Loss: 0.40134866535663605, Validation Loss: 0.398516446352005\n",
      "Epoch 440: Training Loss: 0.4013061076402664, Validation Loss: 0.398441880941391\n",
      "Epoch 441: Training Loss: 0.4012557069460551, Validation Loss: 0.3983255922794342\n",
      "Epoch 442: Training Loss: 0.4012577186028163, Validation Loss: 0.39856502413749695\n",
      "Epoch 443: Training Loss: 0.4013093014558156, Validation Loss: 0.39838069677352905\n",
      "Epoch 444: Training Loss: 0.4014478474855423, Validation Loss: 0.39799365401268005\n",
      "Epoch 445: Training Loss: 0.4012268930673599, Validation Loss: 0.3986750543117523\n",
      "Epoch 446: Training Loss: 0.40121253828207654, Validation Loss: 0.39880335330963135\n",
      "Epoch 447: Training Loss: 0.4012734939654668, Validation Loss: 0.3981640338897705\n",
      "Epoch 448: Training Loss: 0.40125714242458344, Validation Loss: 0.3987269401550293\n",
      "Epoch 449: Training Loss: 0.4011782457431157, Validation Loss: 0.39822569489479065\n",
      "Epoch 450: Training Loss: 0.40116675198078156, Validation Loss: 0.3982143998146057\n",
      "Epoch 451: Training Loss: 0.4011465708414714, Validation Loss: 0.3985682725906372\n",
      "Epoch 452: Training Loss: 0.4011579652627309, Validation Loss: 0.3982039988040924\n",
      "Epoch 453: Training Loss: 0.4011267274618149, Validation Loss: 0.3986523747444153\n",
      "Epoch 454: Training Loss: 0.40108907719453174, Validation Loss: 0.3985190987586975\n",
      "Epoch 455: Training Loss: 0.40107908844947815, Validation Loss: 0.39815792441368103\n",
      "Epoch 456: Training Loss: 0.40119512379169464, Validation Loss: 0.39780351519584656\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 457: Training Loss: 0.40116341412067413, Validation Loss: 0.3987412452697754\n",
      "Epoch 458: Training Loss: 0.4011144240697225, Validation Loss: 0.39856743812561035\n",
      "Epoch 459: Training Loss: 0.401054913798968, Validation Loss: 0.39798957109451294\n",
      "Epoch 460: Training Loss: 0.401041512688001, Validation Loss: 0.3982226550579071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 461: Training Loss: 0.4011109073956807, Validation Loss: 0.3988085687160492\n",
      "Epoch 462: Training Loss: 0.40107062955697376, Validation Loss: 0.3980521559715271\n",
      "Epoch 463: Training Loss: 0.401037593682607, Validation Loss: 0.3982940912246704\n",
      "Epoch 464: Training Loss: 0.400951623916626, Validation Loss: 0.3986548185348511\n",
      "Epoch 465: Training Loss: 0.4010300536950429, Validation Loss: 0.3985241651535034\n",
      "Epoch 466: Training Loss: 0.40133637686570484, Validation Loss: 0.3975291848182678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 467: Training Loss: 0.40111425518989563, Validation Loss: 0.39834344387054443\n",
      "Epoch 468: Training Loss: 0.4010000377893448, Validation Loss: 0.39932388067245483\n",
      "Epoch 469: Training Loss: 0.4011830538511276, Validation Loss: 0.39924347400665283\n",
      "Epoch 470: Training Loss: 0.4010096341371536, Validation Loss: 0.3978550136089325\n",
      "Epoch 471: Training Loss: 0.40097397565841675, Validation Loss: 0.3979995548725128\n",
      "Epoch 472: Training Loss: 0.40087518095970154, Validation Loss: 0.3989362120628357\n",
      "Epoch 473: Training Loss: 0.4009074370066325, Validation Loss: 0.3985130786895752\n",
      "Epoch 474: Training Loss: 0.4009677569071452, Validation Loss: 0.39797359704971313\n",
      "Epoch 475: Training Loss: 0.4008789410193761, Validation Loss: 0.39861372113227844\n",
      "Epoch 476: Training Loss: 0.4008929580450058, Validation Loss: 0.3986431360244751\n",
      "Epoch 477: Training Loss: 0.400798241297404, Validation Loss: 0.3981592059135437\n",
      "Epoch 478: Training Loss: 0.4008328566948573, Validation Loss: 0.3977801501750946\n",
      "Epoch 479: Training Loss: 0.4007748415072759, Validation Loss: 0.39836210012435913\n",
      "Epoch 480: Training Loss: 0.400737186272939, Validation Loss: 0.3987341821193695\n",
      "Epoch 481: Training Loss: 0.4008140116930008, Validation Loss: 0.3982534110546112\n",
      "Epoch 482: Training Loss: 0.40094808240731555, Validation Loss: 0.39747026562690735\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 483: Training Loss: 0.4008413950602214, Validation Loss: 0.3983710706233978\n",
      "Epoch 484: Training Loss: 0.40083470443884534, Validation Loss: 0.399181991815567\n",
      "Epoch 485: Training Loss: 0.4007745285828908, Validation Loss: 0.39794209599494934\n",
      "Epoch 486: Training Loss: 0.4006455938021342, Validation Loss: 0.39767155051231384\n",
      "Epoch 487: Training Loss: 0.40076622863610584, Validation Loss: 0.398062527179718\n",
      "Epoch 488: Training Loss: 0.4006824443737666, Validation Loss: 0.398771733045578\n",
      "Epoch 489: Training Loss: 0.40068726738293964, Validation Loss: 0.39854001998901367\n",
      "Epoch 490: Training Loss: 0.4007967362801234, Validation Loss: 0.3980115056037903\n",
      "Epoch 491: Training Loss: 0.40069297949473065, Validation Loss: 0.39860233664512634\n",
      "Epoch 492: Training Loss: 0.4006694555282593, Validation Loss: 0.39844444394111633\n",
      "Epoch 493: Training Loss: 0.4005640546480815, Validation Loss: 0.39783036708831787\n",
      "Epoch 494: Training Loss: 0.40060340861479443, Validation Loss: 0.39790162444114685\n",
      "Epoch 495: Training Loss: 0.4005727569262187, Validation Loss: 0.398384153842926\n",
      "Epoch 496: Training Loss: 0.4005880703528722, Validation Loss: 0.39848652482032776\n",
      "Epoch 497: Training Loss: 0.4006567398707072, Validation Loss: 0.39902377128601074\n",
      "Epoch 498: Training Loss: 0.4006602317094803, Validation Loss: 0.39821699261665344\n",
      "Epoch 499: Training Loss: 0.4005216012398402, Validation Loss: 0.398383766412735\n",
      "Epoch 500: Training Loss: 0.40063977738221485, Validation Loss: 0.3980279862880707\n",
      "Epoch 501: Training Loss: 0.4007427344719569, Validation Loss: 0.3988162875175476\n",
      "Epoch 502: Training Loss: 0.4006012926499049, Validation Loss: 0.3976973593235016\n",
      "Epoch 503: Training Loss: 0.40049752593040466, Validation Loss: 0.3981347978115082\n",
      "Epoch 504: Training Loss: 0.40044061342875165, Validation Loss: 0.39846736192703247\n",
      "Epoch 505: Training Loss: 0.4005492627620697, Validation Loss: 0.3990420699119568\n",
      "Epoch 506: Training Loss: 0.40043819944063824, Validation Loss: 0.39801448583602905\n",
      "Epoch 507: Training Loss: 0.4004313498735428, Validation Loss: 0.3979911506175995\n",
      "Epoch 508: Training Loss: 0.40051573018232983, Validation Loss: 0.39822736382484436\n",
      "Epoch 509: Training Loss: 0.4004010359446208, Validation Loss: 0.3988136947154999\n",
      "Epoch 510: Training Loss: 0.4008706907431285, Validation Loss: 0.39993155002593994\n",
      "Epoch 511: Training Loss: 0.40050602952639264, Validation Loss: 0.3978629410266876\n",
      "Epoch 512: Training Loss: 0.4007435192664464, Validation Loss: 0.39745789766311646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 513: Training Loss: 0.4006333698829015, Validation Loss: 0.3982568085193634\n",
      "Epoch 514: Training Loss: 0.40046873191992444, Validation Loss: 0.3999694883823395\n",
      "Epoch 515: Training Loss: 0.40058941145737964, Validation Loss: 0.39847737550735474\n",
      "Epoch 516: Training Loss: 0.4003819525241852, Validation Loss: 0.3977503180503845\n",
      "Epoch 517: Training Loss: 0.40046977003415424, Validation Loss: 0.398224800825119\n",
      "Epoch 518: Training Loss: 0.40028974910577136, Validation Loss: 0.39844903349876404\n",
      "Epoch 519: Training Loss: 0.40031500657399494, Validation Loss: 0.3984520435333252\n",
      "Epoch 520: Training Loss: 0.4003008504708608, Validation Loss: 0.3978992998600006\n",
      "Epoch 521: Training Loss: 0.4003034432729085, Validation Loss: 0.39798274636268616\n",
      "Epoch 522: Training Loss: 0.4002358814080556, Validation Loss: 0.3987978994846344\n",
      "Epoch 523: Training Loss: 0.4004899362723033, Validation Loss: 0.397844523191452\n",
      "Epoch 524: Training Loss: 0.400400772690773, Validation Loss: 0.39773625135421753\n",
      "Epoch 525: Training Loss: 0.40013687312602997, Validation Loss: 0.39856278896331787\n",
      "Epoch 526: Training Loss: 0.40027731160322827, Validation Loss: 0.39853382110595703\n",
      "Epoch 527: Training Loss: 0.4001875966787338, Validation Loss: 0.3982350826263428\n",
      "Epoch 528: Training Loss: 0.4004059086243312, Validation Loss: 0.3977109491825104\n",
      "Epoch 529: Training Loss: 0.40023145576318103, Validation Loss: 0.3982425630092621\n",
      "Epoch 530: Training Loss: 0.4003460307916005, Validation Loss: 0.39984628558158875\n",
      "Epoch 531: Training Loss: 0.4003619849681854, Validation Loss: 0.3983505964279175\n",
      "Epoch 532: Training Loss: 0.40014291803042096, Validation Loss: 0.39758190512657166\n",
      "Epoch 533: Training Loss: 0.4002523273229599, Validation Loss: 0.3979644775390625\n",
      "Epoch 534: Training Loss: 0.4000808000564575, Validation Loss: 0.3987569212913513\n",
      "Epoch 535: Training Loss: 0.40017127990722656, Validation Loss: 0.3989412784576416\n",
      "Epoch 536: Training Loss: 0.4001578191916148, Validation Loss: 0.3980458080768585\n",
      "Epoch 537: Training Loss: 0.400177538394928, Validation Loss: 0.3978298306465149\n",
      "Epoch 538: Training Loss: 0.4001415769259135, Validation Loss: 0.39799997210502625\n",
      "Epoch 539: Training Loss: 0.40009041627248126, Validation Loss: 0.3980807662010193\n",
      "Epoch 540: Training Loss: 0.40008031328519184, Validation Loss: 0.39881324768066406\n",
      "Epoch 541: Training Loss: 0.4001962294181188, Validation Loss: 0.3981465697288513\n",
      "Epoch 542: Training Loss: 0.4000435223182042, Validation Loss: 0.3986639976501465\n",
      "Epoch 543: Training Loss: 0.3999918947617213, Validation Loss: 0.39848464727401733\n",
      "Epoch 544: Training Loss: 0.39995864530404407, Validation Loss: 0.3981400430202484\n",
      "Epoch 545: Training Loss: 0.4001248776912689, Validation Loss: 0.39792969822883606\n",
      "Epoch 546: Training Loss: 0.4000093291203181, Validation Loss: 0.3987264335155487\n",
      "Epoch 547: Training Loss: 0.4000221937894821, Validation Loss: 0.3985200822353363\n",
      "Epoch 548: Training Loss: 0.3999607264995575, Validation Loss: 0.39772436022758484\n",
      "Epoch 549: Training Loss: 0.3999849061171214, Validation Loss: 0.3980790972709656\n",
      "Epoch 550: Training Loss: 0.40002326170603436, Validation Loss: 0.3985385596752167\n",
      "Epoch 551: Training Loss: 0.39994946618874866, Validation Loss: 0.3979099690914154\n",
      "Epoch 552: Training Loss: 0.39996406932671863, Validation Loss: 0.3980352580547333\n",
      "Epoch 553: Training Loss: 0.39994951089223224, Validation Loss: 0.39810672402381897\n",
      "Epoch 554: Training Loss: 0.3999507377545039, Validation Loss: 0.3990723788738251\n",
      "Epoch 555: Training Loss: 0.3999188145001729, Validation Loss: 0.3981623649597168\n",
      "Epoch 556: Training Loss: 0.3998777319987615, Validation Loss: 0.39810964465141296\n",
      "Epoch 557: Training Loss: 0.39988849063714343, Validation Loss: 0.3982144892215729\n",
      "Epoch 558: Training Loss: 0.3999652812878291, Validation Loss: 0.3991142511367798\n",
      "Epoch 559: Training Loss: 0.4000031103690465, Validation Loss: 0.3979710042476654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 560: Training Loss: 0.3998517046372096, Validation Loss: 0.3982064425945282\n",
      "Epoch 561: Training Loss: 0.3998008966445923, Validation Loss: 0.3981536328792572\n",
      "Epoch 562: Training Loss: 0.399730126063029, Validation Loss: 0.3984760344028473\n",
      "Epoch 563: Training Loss: 0.3998323877652486, Validation Loss: 0.3987268805503845\n",
      "Epoch 564: Training Loss: 0.3997006118297577, Validation Loss: 0.3980017602443695\n",
      "Epoch 565: Training Loss: 0.39988618592421216, Validation Loss: 0.3977890908718109\n",
      "Epoch 566: Training Loss: 0.3997245132923126, Validation Loss: 0.39858943223953247\n",
      "Epoch 567: Training Loss: 0.40032264590263367, Validation Loss: 0.4002234935760498\n",
      "Epoch 568: Training Loss: 0.4000874360402425, Validation Loss: 0.3985477387905121\n",
      "Epoch 569: Training Loss: 0.39986972510814667, Validation Loss: 0.3974646031856537\n",
      "Epoch 570: Training Loss: 0.3999527394771576, Validation Loss: 0.39800697565078735\n",
      "Epoch 571: Training Loss: 0.39956608414649963, Validation Loss: 0.3989930748939514\n",
      "Epoch 572: Training Loss: 0.40009017785390216, Validation Loss: 0.3998905420303345\n",
      "Epoch 573: Training Loss: 0.3998101403315862, Validation Loss: 0.3978117108345032\n",
      "Epoch 574: Training Loss: 0.3999783545732498, Validation Loss: 0.39746740460395813\n",
      "Epoch 575: Training Loss: 0.3999110559622447, Validation Loss: 0.3989224433898926\n",
      "Epoch 576: Training Loss: 0.399949053923289, Validation Loss: 0.3989497423171997\n",
      "Epoch 577: Training Loss: 0.39982756475607556, Validation Loss: 0.39783528447151184\n",
      "Epoch 578: Training Loss: 0.3997453302145004, Validation Loss: 0.39791789650917053\n",
      "Epoch 579: Training Loss: 0.39954810837904614, Validation Loss: 0.3989231288433075\n",
      "Epoch 580: Training Loss: 0.3996656686067581, Validation Loss: 0.3988396227359772\n",
      "Epoch 581: Training Loss: 0.39954766631126404, Validation Loss: 0.39807096123695374\n",
      "Epoch 582: Training Loss: 0.40024856726328534, Validation Loss: 0.3974747955799103\n",
      "Epoch 583: Training Loss: 0.39986129601796466, Validation Loss: 0.3987756073474884\n",
      "Epoch 584: Training Loss: 0.39969952404499054, Validation Loss: 0.3990631699562073\n",
      "Epoch 585: Training Loss: 0.3996434112389882, Validation Loss: 0.3977733552455902\n",
      "Epoch 586: Training Loss: 0.39978361626466113, Validation Loss: 0.3978669047355652\n",
      "Epoch 587: Training Loss: 0.399550919731458, Validation Loss: 0.3991914391517639\n",
      "Epoch 588: Training Loss: 0.3996485372384389, Validation Loss: 0.3988134562969208\n",
      "Epoch 589: Training Loss: 0.3994882603486379, Validation Loss: 0.3983090817928314\n",
      "Epoch 590: Training Loss: 0.3995079348484675, Validation Loss: 0.39820998907089233\n",
      "Epoch 591: Training Loss: 0.3994227200746536, Validation Loss: 0.39880120754241943\n",
      "Epoch 592: Training Loss: 0.3998478601376216, Validation Loss: 0.3996216654777527\n",
      "Epoch 593: Training Loss: 0.39950162172317505, Validation Loss: 0.3978594243526459\n",
      "Epoch 594: Training Loss: 0.39963356653849286, Validation Loss: 0.39750808477401733\n",
      "Epoch 595: Training Loss: 0.3994406114021937, Validation Loss: 0.3987879753112793\n",
      "Epoch 596: Training Loss: 0.39973344405492145, Validation Loss: 0.3997720181941986\n",
      "Epoch 597: Training Loss: 0.39963024854660034, Validation Loss: 0.39838913083076477\n",
      "Epoch 598: Training Loss: 0.3994741241137187, Validation Loss: 0.39800307154655457\n",
      "Epoch 599: Training Loss: 0.3994339307149251, Validation Loss: 0.3983021080493927\n",
      "Epoch 600: Training Loss: 0.3993452191352844, Validation Loss: 0.39843857288360596\n",
      "Epoch 601: Training Loss: 0.39931948979695636, Validation Loss: 0.39866966009140015\n",
      "Epoch 602: Training Loss: 0.3993271787961324, Validation Loss: 0.3983781635761261\n",
      "Epoch 603: Training Loss: 0.39931024114290875, Validation Loss: 0.39818981289863586\n",
      "Epoch 604: Training Loss: 0.39929184317588806, Validation Loss: 0.39852771162986755\n",
      "Epoch 605: Training Loss: 0.3993408977985382, Validation Loss: 0.39832764863967896\n",
      "Epoch 606: Training Loss: 0.3993595093488693, Validation Loss: 0.3981284499168396\n",
      "Epoch 607: Training Loss: 0.3993282715479533, Validation Loss: 0.3985954523086548\n",
      "Epoch 608: Training Loss: 0.3993153969446818, Validation Loss: 0.39816680550575256\n",
      "Epoch 609: Training Loss: 0.399362380305926, Validation Loss: 0.3979519009590149\n",
      "Epoch 610: Training Loss: 0.3993137429157893, Validation Loss: 0.3990268111228943\n",
      "Epoch 611: Training Loss: 0.3992767930030823, Validation Loss: 0.3986039161682129\n",
      "Epoch 612: Training Loss: 0.39920175075531006, Validation Loss: 0.3984105885028839\n",
      "Epoch 613: Training Loss: 0.39920973777770996, Validation Loss: 0.3981156349182129\n",
      "Epoch 614: Training Loss: 0.399305726091067, Validation Loss: 0.39846882224082947\n",
      "Epoch 615: Training Loss: 0.39924722413221997, Validation Loss: 0.3987061381340027\n",
      "Epoch 616: Training Loss: 0.39919400215148926, Validation Loss: 0.39815178513526917\n",
      "Epoch 617: Training Loss: 0.39927753806114197, Validation Loss: 0.39805731177330017\n",
      "Epoch 618: Training Loss: 0.39932451645533246, Validation Loss: 0.3992638885974884\n",
      "Epoch 619: Training Loss: 0.39925268292427063, Validation Loss: 0.3983173072338104\n",
      "Epoch 620: Training Loss: 0.39926958084106445, Validation Loss: 0.3977473974227905\n",
      "Epoch 621: Training Loss: 0.39928679168224335, Validation Loss: 0.39851444959640503\n",
      "Epoch 622: Training Loss: 0.3991246372461319, Validation Loss: 0.39889365434646606\n",
      "Epoch 623: Training Loss: 0.3990938415129979, Validation Loss: 0.3981402814388275\n",
      "Epoch 624: Training Loss: 0.3991943299770355, Validation Loss: 0.39809516072273254\n",
      "Epoch 625: Training Loss: 0.3991636782884598, Validation Loss: 0.399159699678421\n",
      "Epoch 626: Training Loss: 0.3993929972251256, Validation Loss: 0.39916759729385376\n",
      "Epoch 627: Training Loss: 0.39901697138945263, Validation Loss: 0.3978119492530823\n",
      "Epoch 628: Training Loss: 0.39914336800575256, Validation Loss: 0.39797261357307434\n",
      "Epoch 629: Training Loss: 0.3992326309283574, Validation Loss: 0.3992645740509033\n",
      "Epoch 630: Training Loss: 0.3992024014393489, Validation Loss: 0.3985922634601593\n",
      "Epoch 631: Training Loss: 0.39895350237687427, Validation Loss: 0.3982132077217102\n",
      "Epoch 632: Training Loss: 0.3990462472041448, Validation Loss: 0.39818695187568665\n",
      "Epoch 633: Training Loss: 0.3990238606929779, Validation Loss: 0.3986560106277466\n",
      "Epoch 634: Training Loss: 0.3989933629830678, Validation Loss: 0.3989759087562561\n",
      "Epoch 635: Training Loss: 0.39896443982919055, Validation Loss: 0.3984763026237488\n",
      "Epoch 636: Training Loss: 0.3989928911129634, Validation Loss: 0.39826154708862305\n",
      "Epoch 637: Training Loss: 0.3989671965440114, Validation Loss: 0.3989969491958618\n",
      "Epoch 638: Training Loss: 0.3990187495946884, Validation Loss: 0.3989196717739105\n",
      "Epoch 639: Training Loss: 0.3990360001722972, Validation Loss: 0.39857086539268494\n",
      "Epoch 640: Training Loss: 0.3991687595844269, Validation Loss: 0.39761149883270264\n",
      "Epoch 641: Training Loss: 0.399392306804657, Validation Loss: 0.3985714316368103\n",
      "Epoch 642: Training Loss: 0.39893535772959393, Validation Loss: 0.3985159397125244\n",
      "Epoch 643: Training Loss: 0.39907897512118023, Validation Loss: 0.39935922622680664\n",
      "Epoch 644: Training Loss: 0.3988773028055827, Validation Loss: 0.3983045816421509\n",
      "Epoch 645: Training Loss: 0.3990642676750819, Validation Loss: 0.39834296703338623\n",
      "Epoch 646: Training Loss: 0.3988853096961975, Validation Loss: 0.39932385087013245\n",
      "Epoch 647: Training Loss: 0.39909547567367554, Validation Loss: 0.3995327949523926\n",
      "Epoch 648: Training Loss: 0.39885595440864563, Validation Loss: 0.39813536405563354\n",
      "Epoch 649: Training Loss: 0.3989311655362447, Validation Loss: 0.3982487618923187\n",
      "Epoch 650: Training Loss: 0.3988732049862544, Validation Loss: 0.39895784854888916\n",
      "Epoch 651: Training Loss: 0.3988598386446635, Validation Loss: 0.3984908163547516\n",
      "Epoch 652: Training Loss: 0.398810272415479, Validation Loss: 0.3982445001602173\n",
      "Epoch 653: Training Loss: 0.3987632989883423, Validation Loss: 0.39898809790611267\n",
      "Epoch 654: Training Loss: 0.39898255467414856, Validation Loss: 0.39957696199417114\n",
      "Epoch 655: Training Loss: 0.3988751620054245, Validation Loss: 0.39840805530548096\n",
      "Epoch 656: Training Loss: 0.39875794450442, Validation Loss: 0.3985176682472229\n",
      "Epoch 657: Training Loss: 0.3988048732280731, Validation Loss: 0.3990360498428345\n",
      "Epoch 658: Training Loss: 0.39877120157082874, Validation Loss: 0.39833366870880127\n",
      "Epoch 659: Training Loss: 0.398721049229304, Validation Loss: 0.3982475996017456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 660: Training Loss: 0.39868877828121185, Validation Loss: 0.3987908959388733\n",
      "Epoch 661: Training Loss: 0.3986802895863851, Validation Loss: 0.39879000186920166\n",
      "Epoch 662: Training Loss: 0.39864778021971387, Validation Loss: 0.39868515729904175\n",
      "Epoch 663: Training Loss: 0.39867482086022693, Validation Loss: 0.39862701296806335\n",
      "Epoch 664: Training Loss: 0.3987407038609187, Validation Loss: 0.39891350269317627\n",
      "Epoch 665: Training Loss: 0.3986085206270218, Validation Loss: 0.3983064293861389\n",
      "Epoch 666: Training Loss: 0.3986479292313258, Validation Loss: 0.3982376158237457\n",
      "Epoch 667: Training Loss: 0.39869895577430725, Validation Loss: 0.39909598231315613\n",
      "Epoch 668: Training Loss: 0.3987722446521123, Validation Loss: 0.39829689264297485\n",
      "Epoch 669: Training Loss: 0.39860538641611737, Validation Loss: 0.39863741397857666\n",
      "Epoch 670: Training Loss: 0.39856794973214466, Validation Loss: 0.3993240296840668\n",
      "Epoch 671: Training Loss: 0.39863235751787823, Validation Loss: 0.3986020088195801\n",
      "Epoch 672: Training Loss: 0.39855573574701947, Validation Loss: 0.3986797034740448\n",
      "Epoch 673: Training Loss: 0.3985443363587062, Validation Loss: 0.39876508712768555\n",
      "Epoch 674: Training Loss: 0.3985308110713959, Validation Loss: 0.398664265871048\n",
      "Epoch 675: Training Loss: 0.3985356191794078, Validation Loss: 0.3987847566604614\n",
      "Epoch 676: Training Loss: 0.39866282045841217, Validation Loss: 0.3983139097690582\n",
      "Epoch 677: Training Loss: 0.3985532969236374, Validation Loss: 0.39891645312309265\n",
      "Epoch 678: Training Loss: 0.39881377418835956, Validation Loss: 0.39973920583724976\n",
      "Epoch 679: Training Loss: 0.39848830302556354, Validation Loss: 0.39830344915390015\n",
      "Epoch 680: Training Loss: 0.3986418694257736, Validation Loss: 0.3980197608470917\n",
      "Epoch 681: Training Loss: 0.3987758308649063, Validation Loss: 0.39899057149887085\n",
      "Epoch 682: Training Loss: 0.39847048620382947, Validation Loss: 0.39876237511634827\n",
      "Epoch 683: Training Loss: 0.39840440452098846, Validation Loss: 0.39865612983703613\n",
      "Epoch 684: Training Loss: 0.3984889139731725, Validation Loss: 0.39847660064697266\n",
      "Epoch 685: Training Loss: 0.39854519565900165, Validation Loss: 0.39863190054893494\n",
      "Epoch 686: Training Loss: 0.39854273200035095, Validation Loss: 0.399860680103302\n",
      "Epoch 687: Training Loss: 0.39853258927663165, Validation Loss: 0.3989675045013428\n",
      "Epoch 688: Training Loss: 0.3984008977810542, Validation Loss: 0.39855313301086426\n",
      "Epoch 689: Training Loss: 0.3984091579914093, Validation Loss: 0.39880937337875366\n",
      "Epoch 690: Training Loss: 0.3984169115622838, Validation Loss: 0.3986375331878662\n",
      "Epoch 691: Training Loss: 0.39843591550985974, Validation Loss: 0.3990035355091095\n",
      "Epoch 692: Training Loss: 0.39840153356393176, Validation Loss: 0.3986204266548157\n",
      "Epoch 693: Training Loss: 0.39852013687292737, Validation Loss: 0.3981722891330719\n",
      "Epoch 694: Training Loss: 0.3983244200547536, Validation Loss: 0.3991958200931549\n",
      "Epoch 695: Training Loss: 0.39849895735581714, Validation Loss: 0.40005502104759216\n",
      "Epoch 696: Training Loss: 0.3983574757973353, Validation Loss: 0.3985200524330139\n",
      "Epoch 697: Training Loss: 0.3988320181767146, Validation Loss: 0.39811670780181885\n",
      "Epoch 698: Training Loss: 0.39877522985140484, Validation Loss: 0.398837685585022\n",
      "Epoch 699: Training Loss: 0.39857523143291473, Validation Loss: 0.40027010440826416\n",
      "Epoch 700: Training Loss: 0.39838266869386035, Validation Loss: 0.39851129055023193\n",
      "Epoch 701: Training Loss: 0.39829976856708527, Validation Loss: 0.39827829599380493\n",
      "Epoch 702: Training Loss: 0.3982727179924647, Validation Loss: 0.3988807201385498\n",
      "Epoch 703: Training Loss: 0.39822964866956073, Validation Loss: 0.3991958796977997\n",
      "Epoch 704: Training Loss: 0.3983774383862813, Validation Loss: 0.3986615538597107\n",
      "Epoch 705: Training Loss: 0.3982851654291153, Validation Loss: 0.39869168400764465\n",
      "Epoch 706: Training Loss: 0.39816828072071075, Validation Loss: 0.3991736173629761\n",
      "Epoch 707: Training Loss: 0.39835071563720703, Validation Loss: 0.4000360369682312\n",
      "Epoch 708: Training Loss: 0.3982408990462621, Validation Loss: 0.3985729515552521\n",
      "Epoch 709: Training Loss: 0.3983888179063797, Validation Loss: 0.39819276332855225\n",
      "Epoch 710: Training Loss: 0.39829689761002857, Validation Loss: 0.39893391728401184\n",
      "Epoch 711: Training Loss: 0.39802631735801697, Validation Loss: 0.40030917525291443\n",
      "Epoch 712: Training Loss: 0.3985295295715332, Validation Loss: 0.399951696395874\n",
      "Epoch 713: Training Loss: 0.39838556448618573, Validation Loss: 0.39846131205558777\n",
      "Epoch 714: Training Loss: 0.3982195357481639, Validation Loss: 0.39875704050064087\n",
      "Epoch 715: Training Loss: 0.39807021617889404, Validation Loss: 0.3993056118488312\n",
      "Epoch 716: Training Loss: 0.398157075047493, Validation Loss: 0.39916637539863586\n",
      "Epoch 717: Training Loss: 0.3981177856524785, Validation Loss: 0.39949244260787964\n",
      "Epoch 718: Training Loss: 0.3981160372495651, Validation Loss: 0.39912599325180054\n",
      "Epoch 719: Training Loss: 0.39816002051035565, Validation Loss: 0.3991695046424866\n",
      "Epoch 720: Training Loss: 0.39806624750296277, Validation Loss: 0.3989126980304718\n",
      "Epoch 721: Training Loss: 0.3982253869374593, Validation Loss: 0.39840611815452576\n",
      "Epoch 722: Training Loss: 0.39805637300014496, Validation Loss: 0.3993847072124481\n",
      "Epoch 723: Training Loss: 0.3982369899749756, Validation Loss: 0.3998079001903534\n",
      "Epoch 724: Training Loss: 0.3980363557736079, Validation Loss: 0.39853501319885254\n",
      "Epoch 725: Training Loss: 0.39822980761528015, Validation Loss: 0.39848437905311584\n",
      "Epoch 726: Training Loss: 0.397992084423701, Validation Loss: 0.39984241127967834\n",
      "Epoch 727: Training Loss: 0.3981953908999761, Validation Loss: 0.39941152930259705\n",
      "Epoch 728: Training Loss: 0.39792827268441516, Validation Loss: 0.3985658288002014\n",
      "Epoch 729: Training Loss: 0.39797066152095795, Validation Loss: 0.3983754813671112\n",
      "Epoch 730: Training Loss: 0.39812346796194714, Validation Loss: 0.39914175868034363\n",
      "Epoch 731: Training Loss: 0.39791343609491986, Validation Loss: 0.3992738127708435\n",
      "Epoch 732: Training Loss: 0.397971232732137, Validation Loss: 0.3989531099796295\n",
      "Epoch 733: Training Loss: 0.3978757510582606, Validation Loss: 0.3987099826335907\n",
      "Epoch 734: Training Loss: 0.39814087748527527, Validation Loss: 0.39832305908203125\n",
      "Epoch 735: Training Loss: 0.3980845759312312, Validation Loss: 0.39879849553108215\n",
      "Epoch 736: Training Loss: 0.3981258422136307, Validation Loss: 0.400716096162796\n",
      "Epoch 737: Training Loss: 0.3981732875108719, Validation Loss: 0.3996759057044983\n",
      "Epoch 738: Training Loss: 0.3980244994163513, Validation Loss: 0.39846745133399963\n",
      "Epoch 739: Training Loss: 0.39795733491579693, Validation Loss: 0.3991723358631134\n",
      "Epoch 740: Training Loss: 0.3979772130648295, Validation Loss: 0.39935633540153503\n",
      "Epoch 741: Training Loss: 0.39789241552352905, Validation Loss: 0.3983308970928192\n",
      "Epoch 742: Training Loss: 0.39787644147872925, Validation Loss: 0.39871782064437866\n",
      "Epoch 743: Training Loss: 0.39808357258637744, Validation Loss: 0.399891197681427\n",
      "Epoch 744: Training Loss: 0.39788418511549634, Validation Loss: 0.39907610416412354\n",
      "Epoch 745: Training Loss: 0.397835115591685, Validation Loss: 0.3988475203514099\n",
      "Epoch 746: Training Loss: 0.3978087951739629, Validation Loss: 0.39949727058410645\n",
      "Epoch 747: Training Loss: 0.3978538413842519, Validation Loss: 0.3992444574832916\n",
      "Epoch 748: Training Loss: 0.3978407432635625, Validation Loss: 0.3991238474845886\n",
      "Epoch 749: Training Loss: 0.3977857679128647, Validation Loss: 0.3986506462097168\n",
      "Epoch 750: Training Loss: 0.3978210339943568, Validation Loss: 0.3994061052799225\n",
      "Epoch 751: Training Loss: 0.3978575716416041, Validation Loss: 0.3992656171321869\n",
      "Epoch 752: Training Loss: 0.3977867563565572, Validation Loss: 0.39847853779792786\n",
      "Epoch 753: Training Loss: 0.3977924734354019, Validation Loss: 0.3989504277706146\n",
      "Epoch 754: Training Loss: 0.3976214875777562, Validation Loss: 0.399874746799469\n",
      "Epoch 755: Training Loss: 0.39809716741244, Validation Loss: 0.4001019299030304\n",
      "Epoch 756: Training Loss: 0.3978984206914902, Validation Loss: 0.3984783887863159\n",
      "Epoch 757: Training Loss: 0.39776861170927685, Validation Loss: 0.39884698390960693\n",
      "Epoch 758: Training Loss: 0.3977154443661372, Validation Loss: 0.399911493062973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 759: Training Loss: 0.3976922780275345, Validation Loss: 0.39904025197029114\n",
      "Epoch 760: Training Loss: 0.3976878672838211, Validation Loss: 0.3985278010368347\n",
      "Epoch 761: Training Loss: 0.39772536357243854, Validation Loss: 0.39934444427490234\n",
      "Epoch 762: Training Loss: 0.39765264093875885, Validation Loss: 0.3997066617012024\n",
      "Epoch 763: Training Loss: 0.3976994107166926, Validation Loss: 0.3992370367050171\n",
      "Epoch 764: Training Loss: 0.3980129808187485, Validation Loss: 0.3984557092189789\n",
      "Epoch 765: Training Loss: 0.3977329085270564, Validation Loss: 0.39919325709342957\n",
      "Epoch 766: Training Loss: 0.3978260010480881, Validation Loss: 0.40105605125427246\n",
      "Epoch 767: Training Loss: 0.3978894352912903, Validation Loss: 0.3995737135410309\n",
      "Epoch 768: Training Loss: 0.3975975116093953, Validation Loss: 0.3988422155380249\n",
      "Epoch 769: Training Loss: 0.39766255517800647, Validation Loss: 0.3995305299758911\n",
      "Epoch 770: Training Loss: 0.39762664834658307, Validation Loss: 0.39902833104133606\n",
      "Epoch 771: Training Loss: 0.39748627444108325, Validation Loss: 0.3992898762226105\n",
      "Epoch 772: Training Loss: 0.3976271152496338, Validation Loss: 0.3990987241268158\n",
      "Epoch 773: Training Loss: 0.3974876304467519, Validation Loss: 0.3989782929420471\n",
      "Epoch 774: Training Loss: 0.39747894803682965, Validation Loss: 0.39932042360305786\n",
      "Epoch 775: Training Loss: 0.39758630593617755, Validation Loss: 0.3988591730594635\n",
      "Epoch 776: Training Loss: 0.39746936162312824, Validation Loss: 0.3993014097213745\n",
      "Epoch 777: Training Loss: 0.3975358158349991, Validation Loss: 0.39893442392349243\n",
      "Epoch 778: Training Loss: 0.39750077327092487, Validation Loss: 0.39940571784973145\n",
      "Epoch 779: Training Loss: 0.3974778602520625, Validation Loss: 0.39942339062690735\n",
      "Epoch 780: Training Loss: 0.39754058917363483, Validation Loss: 0.3993840515613556\n",
      "Epoch 781: Training Loss: 0.3974015812079112, Validation Loss: 0.39872488379478455\n",
      "Epoch 782: Training Loss: 0.3975127786397934, Validation Loss: 0.3989826738834381\n",
      "Epoch 783: Training Loss: 0.39769185582796734, Validation Loss: 0.4000280797481537\n",
      "Epoch 784: Training Loss: 0.39761286477247876, Validation Loss: 0.39996209740638733\n",
      "Epoch 785: Training Loss: 0.3973953227202098, Validation Loss: 0.3993705213069916\n",
      "Epoch 786: Training Loss: 0.39740221699078876, Validation Loss: 0.39928942918777466\n",
      "Epoch 787: Training Loss: 0.3974195917447408, Validation Loss: 0.3993607759475708\n",
      "Epoch 788: Training Loss: 0.39732589324315387, Validation Loss: 0.39961665868759155\n",
      "Epoch 789: Training Loss: 0.3974351833264033, Validation Loss: 0.39964091777801514\n",
      "Epoch 790: Training Loss: 0.3973728468020757, Validation Loss: 0.3990046977996826\n",
      "Epoch 791: Training Loss: 0.3974240372578303, Validation Loss: 0.39908334612846375\n",
      "Epoch 792: Training Loss: 0.3973434468110402, Validation Loss: 0.39901840686798096\n",
      "Epoch 793: Training Loss: 0.3973573495944341, Validation Loss: 0.39941123127937317\n",
      "Epoch 794: Training Loss: 0.3972853521505992, Validation Loss: 0.40004462003707886\n",
      "Epoch 795: Training Loss: 0.3974649906158447, Validation Loss: 0.3998483419418335\n",
      "Epoch 796: Training Loss: 0.39724848171075183, Validation Loss: 0.3988383114337921\n",
      "Epoch 797: Training Loss: 0.39749757448832196, Validation Loss: 0.3988102674484253\n",
      "Epoch 798: Training Loss: 0.3972155898809433, Validation Loss: 0.39981964230537415\n",
      "Epoch 799: Training Loss: 0.39735407133897144, Validation Loss: 0.400238037109375\n",
      "Epoch 800: Training Loss: 0.39725202818711597, Validation Loss: 0.39899951219558716\n",
      "Epoch 801: Training Loss: 0.39733079572518665, Validation Loss: 0.39887189865112305\n",
      "Epoch 802: Training Loss: 0.39734211564064026, Validation Loss: 0.3998895287513733\n",
      "Epoch 803: Training Loss: 0.39750225841999054, Validation Loss: 0.4014139175415039\n",
      "Epoch 804: Training Loss: 0.39748087525367737, Validation Loss: 0.399312287569046\n",
      "Epoch 805: Training Loss: 0.39772804578145343, Validation Loss: 0.39833691716194153\n",
      "Epoch 806: Training Loss: 0.39754824837048847, Validation Loss: 0.3993903398513794\n",
      "Epoch 807: Training Loss: 0.3974738170703252, Validation Loss: 0.40060973167419434\n",
      "Epoch 808: Training Loss: 0.3973708351453145, Validation Loss: 0.3991338908672333\n",
      "Epoch 809: Training Loss: 0.3972742209831874, Validation Loss: 0.39934012293815613\n",
      "Epoch 810: Training Loss: 0.3972429682811101, Validation Loss: 0.3988202214241028\n",
      "Epoch 811: Training Loss: 0.39710185925165814, Validation Loss: 0.39950668811798096\n",
      "Epoch 812: Training Loss: 0.3972681264082591, Validation Loss: 0.40017688274383545\n",
      "Epoch 813: Training Loss: 0.3974042187134425, Validation Loss: 0.3993850350379944\n",
      "Epoch 814: Training Loss: 0.3973913788795471, Validation Loss: 0.3994075655937195\n",
      "Epoch 815: Training Loss: 0.39712407688299817, Validation Loss: 0.39998453855514526\n",
      "Epoch 816: Training Loss: 0.3970951090256373, Validation Loss: 0.3994218111038208\n",
      "Epoch 817: Training Loss: 0.3971923490365346, Validation Loss: 0.39885520935058594\n",
      "Epoch 818: Training Loss: 0.39713457723458606, Validation Loss: 0.39914563298225403\n",
      "Epoch 819: Training Loss: 0.39706521729628247, Validation Loss: 0.39954882860183716\n",
      "Epoch 820: Training Loss: 0.3971135566631953, Validation Loss: 0.3996480405330658\n",
      "Epoch 821: Training Loss: 0.3970343271891276, Validation Loss: 0.39982137084007263\n",
      "Epoch 822: Training Loss: 0.39718128244082135, Validation Loss: 0.40030401945114136\n",
      "Epoch 823: Training Loss: 0.39709992210070294, Validation Loss: 0.3991585075855255\n",
      "Epoch 824: Training Loss: 0.3970363636811574, Validation Loss: 0.39904019236564636\n",
      "Epoch 825: Training Loss: 0.39723984400431317, Validation Loss: 0.3992345631122589\n",
      "Epoch 826: Training Loss: 0.39727001388867694, Validation Loss: 0.4007892608642578\n",
      "Epoch 827: Training Loss: 0.39714271823565167, Validation Loss: 0.39940646290779114\n",
      "Epoch 828: Training Loss: 0.3969820936520894, Validation Loss: 0.39956462383270264\n",
      "Epoch 829: Training Loss: 0.3969319413105647, Validation Loss: 0.39975133538246155\n",
      "Epoch 830: Training Loss: 0.39696555336316425, Validation Loss: 0.39991408586502075\n",
      "Epoch 831: Training Loss: 0.3970280985037486, Validation Loss: 0.39958688616752625\n",
      "Epoch 832: Training Loss: 0.3970992664496104, Validation Loss: 0.3986659049987793\n",
      "Epoch 833: Training Loss: 0.39712126553058624, Validation Loss: 0.39965593814849854\n",
      "Epoch 834: Training Loss: 0.3968936155239741, Validation Loss: 0.39983153343200684\n",
      "Epoch 835: Training Loss: 0.39689901471138, Validation Loss: 0.3995128870010376\n",
      "Epoch 836: Training Loss: 0.3969092716773351, Validation Loss: 0.3997526466846466\n",
      "Epoch 837: Training Loss: 0.39699507256348926, Validation Loss: 0.3997957706451416\n",
      "Epoch 838: Training Loss: 0.3968410740296046, Validation Loss: 0.40008676052093506\n",
      "Epoch 839: Training Loss: 0.39685555795828503, Validation Loss: 0.39946964383125305\n",
      "Epoch 840: Training Loss: 0.3968735138575236, Validation Loss: 0.3993717432022095\n",
      "Epoch 841: Training Loss: 0.39682579537232715, Validation Loss: 0.3996967375278473\n",
      "Epoch 842: Training Loss: 0.3968446801106135, Validation Loss: 0.39984095096588135\n",
      "Epoch 843: Training Loss: 0.3971322824557622, Validation Loss: 0.40019869804382324\n",
      "Epoch 844: Training Loss: 0.3968645880619685, Validation Loss: 0.39900481700897217\n",
      "Epoch 845: Training Loss: 0.3968830207983653, Validation Loss: 0.39951854944229126\n",
      "Epoch 846: Training Loss: 0.39683452248573303, Validation Loss: 0.4005208909511566\n",
      "Epoch 847: Training Loss: 0.39691666265328723, Validation Loss: 0.3994666039943695\n",
      "Epoch 848: Training Loss: 0.39684419830640155, Validation Loss: 0.4000278413295746\n",
      "Epoch 849: Training Loss: 0.3967470129330953, Validation Loss: 0.39982321858406067\n",
      "Epoch 850: Training Loss: 0.3967209806044896, Validation Loss: 0.39949679374694824\n",
      "Epoch 851: Training Loss: 0.39683760205904645, Validation Loss: 0.3991624414920807\n",
      "Epoch 852: Training Loss: 0.39676254987716675, Validation Loss: 0.3995991051197052\n",
      "Epoch 853: Training Loss: 0.39693013827006024, Validation Loss: 0.4010589122772217\n",
      "Epoch 854: Training Loss: 0.39676879346370697, Validation Loss: 0.399847149848938\n",
      "Epoch 855: Training Loss: 0.39675938089688617, Validation Loss: 0.39994874596595764\n",
      "Epoch 856: Training Loss: 0.39693954090277356, Validation Loss: 0.3997844457626343\n",
      "Epoch 857: Training Loss: 0.3966077317794164, Validation Loss: 0.4004896581172943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 858: Training Loss: 0.3968452264865239, Validation Loss: 0.40054237842559814\n",
      "Epoch 859: Training Loss: 0.39691051344076794, Validation Loss: 0.3996225893497467\n",
      "Epoch 860: Training Loss: 0.3968703548113505, Validation Loss: 0.400240957736969\n",
      "Epoch 861: Training Loss: 0.3968134969472885, Validation Loss: 0.39911213517189026\n",
      "Epoch 862: Training Loss: 0.39671102662881214, Validation Loss: 0.3996264636516571\n",
      "Epoch 863: Training Loss: 0.3967619736989339, Validation Loss: 0.4006187617778778\n",
      "Epoch 864: Training Loss: 0.3965950707594554, Validation Loss: 0.40004631876945496\n",
      "Epoch 865: Training Loss: 0.396675040324529, Validation Loss: 0.4000769257545471\n",
      "Epoch 866: Training Loss: 0.39661258459091187, Validation Loss: 0.39997437596321106\n",
      "Epoch 867: Training Loss: 0.39660412073135376, Validation Loss: 0.39983266592025757\n",
      "Epoch 868: Training Loss: 0.3965820372104645, Validation Loss: 0.39960435032844543\n",
      "Epoch 869: Training Loss: 0.3968258599440257, Validation Loss: 0.399431437253952\n",
      "Epoch 870: Training Loss: 0.39662522574265796, Validation Loss: 0.4002765715122223\n",
      "Epoch 871: Training Loss: 0.3965771347284317, Validation Loss: 0.40102288126945496\n",
      "Epoch 872: Training Loss: 0.39667483667532605, Validation Loss: 0.39983946084976196\n",
      "Epoch 873: Training Loss: 0.3964998076359431, Validation Loss: 0.39949673414230347\n",
      "Epoch 874: Training Loss: 0.3966423918803533, Validation Loss: 0.40022143721580505\n",
      "Epoch 875: Training Loss: 0.3965695798397064, Validation Loss: 0.3998972177505493\n",
      "Epoch 876: Training Loss: 0.3965614189704259, Validation Loss: 0.399524450302124\n",
      "Epoch 877: Training Loss: 0.39652180174986523, Validation Loss: 0.40018337965011597\n",
      "Epoch 878: Training Loss: 0.396480197707812, Validation Loss: 0.4000771939754486\n",
      "Epoch 879: Training Loss: 0.39646200835704803, Validation Loss: 0.3997582197189331\n",
      "Epoch 880: Training Loss: 0.39660537242889404, Validation Loss: 0.39992547035217285\n",
      "Epoch 881: Training Loss: 0.3967016637325287, Validation Loss: 0.39946720004081726\n",
      "Epoch 882: Training Loss: 0.3964038888613383, Validation Loss: 0.4006810188293457\n",
      "Epoch 883: Training Loss: 0.39650458097457886, Validation Loss: 0.4004497230052948\n",
      "Epoch 884: Training Loss: 0.39641904334227246, Validation Loss: 0.3998658061027527\n",
      "Epoch 885: Training Loss: 0.3964686393737793, Validation Loss: 0.4002033770084381\n",
      "Epoch 886: Training Loss: 0.3966025114059448, Validation Loss: 0.4012805223464966\n",
      "Epoch 887: Training Loss: 0.3964465608199437, Validation Loss: 0.4001108407974243\n",
      "Epoch 888: Training Loss: 0.3963949034611384, Validation Loss: 0.3993566930294037\n",
      "Epoch 889: Training Loss: 0.39667028188705444, Validation Loss: 0.3999059200286865\n",
      "Epoch 890: Training Loss: 0.3965107798576355, Validation Loss: 0.40133386850357056\n",
      "Epoch 891: Training Loss: 0.3964663048585256, Validation Loss: 0.4001982510089874\n",
      "Epoch 892: Training Loss: 0.3965678761402766, Validation Loss: 0.39957427978515625\n",
      "Epoch 893: Training Loss: 0.39659637709458667, Validation Loss: 0.4002646505832672\n",
      "Epoch 894: Training Loss: 0.396352822581927, Validation Loss: 0.40063604712486267\n",
      "Epoch 895: Training Loss: 0.39635580281416577, Validation Loss: 0.40010014176368713\n",
      "Epoch 896: Training Loss: 0.3962879379590352, Validation Loss: 0.4001849293708801\n",
      "Epoch 897: Training Loss: 0.39633286992708844, Validation Loss: 0.4002264142036438\n",
      "Epoch 898: Training Loss: 0.3962801347176234, Validation Loss: 0.40051788091659546\n",
      "Epoch 899: Training Loss: 0.3964199523131053, Validation Loss: 0.40094301104545593\n",
      "Epoch 900: Training Loss: 0.39624789357185364, Validation Loss: 0.39992544054985046\n",
      "Epoch 901: Training Loss: 0.39640429119269055, Validation Loss: 0.3997640609741211\n",
      "Epoch 902: Training Loss: 0.3962534765402476, Validation Loss: 0.4007447361946106\n",
      "Epoch 903: Training Loss: 0.3962950458129247, Validation Loss: 0.4006871283054352\n",
      "Epoch 904: Training Loss: 0.39636873702208203, Validation Loss: 0.39966633915901184\n",
      "Epoch 905: Training Loss: 0.396298810839653, Validation Loss: 0.40053075551986694\n",
      "Epoch 906: Training Loss: 0.3963034351666768, Validation Loss: 0.4008755087852478\n",
      "Epoch 907: Training Loss: 0.39623389144738513, Validation Loss: 0.4001997411251068\n",
      "Epoch 908: Training Loss: 0.3963387260834376, Validation Loss: 0.4000815749168396\n",
      "Epoch 909: Training Loss: 0.3963458885749181, Validation Loss: 0.40086737275123596\n",
      "Epoch 910: Training Loss: 0.39625729123751324, Validation Loss: 0.4000340700149536\n",
      "Epoch 911: Training Loss: 0.3961798499027888, Validation Loss: 0.40017807483673096\n",
      "Epoch 912: Training Loss: 0.3963467975457509, Validation Loss: 0.40027639269828796\n",
      "Epoch 913: Training Loss: 0.396172101298968, Validation Loss: 0.40104031562805176\n",
      "Epoch 914: Training Loss: 0.3961418519417445, Validation Loss: 0.40150851011276245\n",
      "Epoch 915: Training Loss: 0.39628661672274273, Validation Loss: 0.4009322226047516\n",
      "Epoch 916: Training Loss: 0.3961406300465266, Validation Loss: 0.40025362372398376\n",
      "Epoch 917: Training Loss: 0.3961932311455409, Validation Loss: 0.4004926383495331\n",
      "Epoch 918: Training Loss: 0.3961830288171768, Validation Loss: 0.40070784091949463\n",
      "Epoch 919: Training Loss: 0.3962533374627431, Validation Loss: 0.40049025416374207\n",
      "Epoch 920: Training Loss: 0.3961394528547923, Validation Loss: 0.4000646471977234\n",
      "Epoch 921: Training Loss: 0.39614711205164593, Validation Loss: 0.40083208680152893\n",
      "Epoch 922: Training Loss: 0.39612188438574475, Validation Loss: 0.4011503756046295\n",
      "Epoch 923: Training Loss: 0.3962089618047078, Validation Loss: 0.40026378631591797\n",
      "Epoch 924: Training Loss: 0.3960812836885452, Validation Loss: 0.40037256479263306\n",
      "Epoch 925: Training Loss: 0.396139994263649, Validation Loss: 0.40079325437545776\n",
      "Epoch 926: Training Loss: 0.3961169123649597, Validation Loss: 0.40038418769836426\n",
      "Epoch 927: Training Loss: 0.39615610241889954, Validation Loss: 0.40031906962394714\n",
      "Epoch 928: Training Loss: 0.3960537314414978, Validation Loss: 0.4012606143951416\n",
      "Epoch 929: Training Loss: 0.3960927178462346, Validation Loss: 0.4004784822463989\n",
      "Epoch 930: Training Loss: 0.3962123394012451, Validation Loss: 0.40000495314598083\n",
      "Epoch 931: Training Loss: 0.3961373915274938, Validation Loss: 0.4006485939025879\n",
      "Epoch 932: Training Loss: 0.3959815551837285, Validation Loss: 0.4005512297153473\n",
      "Epoch 933: Training Loss: 0.3962416797876358, Validation Loss: 0.3999182879924774\n",
      "Epoch 934: Training Loss: 0.39612166086832684, Validation Loss: 0.40019896626472473\n",
      "Epoch 935: Training Loss: 0.39608576397101086, Validation Loss: 0.40154120326042175\n",
      "Epoch 936: Training Loss: 0.3961642732222875, Validation Loss: 0.4011051654815674\n",
      "Epoch 937: Training Loss: 0.3961079915364583, Validation Loss: 0.40089836716651917\n",
      "Epoch 938: Training Loss: 0.396073450644811, Validation Loss: 0.401510626077652\n",
      "Epoch 939: Training Loss: 0.3961402376492818, Validation Loss: 0.4003785252571106\n",
      "Epoch 940: Training Loss: 0.39606840411822003, Validation Loss: 0.4002092778682709\n",
      "Epoch 941: Training Loss: 0.39597542583942413, Validation Loss: 0.4009808301925659\n",
      "Epoch 942: Training Loss: 0.3959975888331731, Validation Loss: 0.4018990099430084\n",
      "Epoch 943: Training Loss: 0.3959846943616867, Validation Loss: 0.4003832936286926\n",
      "Epoch 944: Training Loss: 0.3965006073315938, Validation Loss: 0.3999037742614746\n",
      "Epoch 945: Training Loss: 0.3963824411233266, Validation Loss: 0.40111398696899414\n",
      "Epoch 946: Training Loss: 0.396309698621432, Validation Loss: 0.4026278257369995\n",
      "Epoch 947: Training Loss: 0.39620332419872284, Validation Loss: 0.4005046784877777\n",
      "Epoch 948: Training Loss: 0.39588986337184906, Validation Loss: 0.4008371829986572\n",
      "Epoch 949: Training Loss: 0.3958959182103475, Validation Loss: 0.40150129795074463\n",
      "Epoch 950: Training Loss: 0.3959082116683324, Validation Loss: 0.4009905755519867\n",
      "Epoch 951: Training Loss: 0.3959658642609914, Validation Loss: 0.4006378948688507\n",
      "Epoch 952: Training Loss: 0.39587647716204327, Validation Loss: 0.40098699927330017\n",
      "Epoch 953: Training Loss: 0.39587519069512683, Validation Loss: 0.4008902907371521\n",
      "Epoch 954: Training Loss: 0.39584674934546155, Validation Loss: 0.4005447328090668\n",
      "Epoch 955: Training Loss: 0.39583465953667957, Validation Loss: 0.40132901072502136\n",
      "Epoch 956: Training Loss: 0.3958174337943395, Validation Loss: 0.4015392065048218\n",
      "Epoch 957: Training Loss: 0.3958241244157155, Validation Loss: 0.40095600485801697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 958: Training Loss: 0.3957443485657374, Validation Loss: 0.4011237323284149\n",
      "Epoch 959: Training Loss: 0.39576101303100586, Validation Loss: 0.4009073078632355\n",
      "Epoch 960: Training Loss: 0.3957507212956746, Validation Loss: 0.40054789185523987\n",
      "Epoch 961: Training Loss: 0.39602990945180255, Validation Loss: 0.4002831280231476\n",
      "Epoch 962: Training Loss: 0.39620358248551685, Validation Loss: 0.4019525647163391\n",
      "Epoch 963: Training Loss: 0.3960558424393336, Validation Loss: 0.4002382755279541\n",
      "Epoch 964: Training Loss: 0.39598608513673145, Validation Loss: 0.4003579318523407\n",
      "Epoch 965: Training Loss: 0.3960232436656952, Validation Loss: 0.4020877182483673\n",
      "Epoch 966: Training Loss: 0.39575642844041187, Validation Loss: 0.40110254287719727\n",
      "Epoch 967: Training Loss: 0.3962508688370387, Validation Loss: 0.4003015458583832\n",
      "Epoch 968: Training Loss: 0.3960717022418976, Validation Loss: 0.401302695274353\n",
      "Epoch 969: Training Loss: 0.39581264555454254, Validation Loss: 0.402616411447525\n",
      "Epoch 970: Training Loss: 0.3960361232360204, Validation Loss: 0.40152522921562195\n",
      "Epoch 971: Training Loss: 0.39560435712337494, Validation Loss: 0.40058693289756775\n",
      "Epoch 972: Training Loss: 0.3960546652475993, Validation Loss: 0.4001196324825287\n",
      "Epoch 973: Training Loss: 0.3957602580388387, Validation Loss: 0.4016100764274597\n",
      "Epoch 974: Training Loss: 0.39584948619206745, Validation Loss: 0.4010179936885834\n",
      "Epoch 975: Training Loss: 0.39567990601062775, Validation Loss: 0.4004639983177185\n",
      "Epoch 976: Training Loss: 0.3957047114769618, Validation Loss: 0.40142735838890076\n",
      "Epoch 977: Training Loss: 0.39574290812015533, Validation Loss: 0.40219682455062866\n",
      "Epoch 978: Training Loss: 0.39577946563561756, Validation Loss: 0.40107354521751404\n",
      "Epoch 979: Training Loss: 0.3956235349178314, Validation Loss: 0.40119728446006775\n",
      "Epoch 980: Training Loss: 0.3955988536278407, Validation Loss: 0.4010533094406128\n",
      "Epoch 981: Training Loss: 0.3956049134333928, Validation Loss: 0.40079453587532043\n",
      "Epoch 982: Training Loss: 0.39555441836516064, Validation Loss: 0.40076637268066406\n",
      "Epoch 983: Training Loss: 0.3957863400379817, Validation Loss: 0.40087100863456726\n",
      "Epoch 984: Training Loss: 0.39558997253576916, Validation Loss: 0.4019884765148163\n",
      "Epoch 985: Training Loss: 0.39560437699158985, Validation Loss: 0.40129896998405457\n",
      "Epoch 986: Training Loss: 0.39554164310296375, Validation Loss: 0.40097394585609436\n",
      "Epoch 987: Training Loss: 0.3955975224574407, Validation Loss: 0.40112486481666565\n",
      "Epoch 988: Training Loss: 0.3955843200286229, Validation Loss: 0.40112432837486267\n",
      "Epoch 989: Training Loss: 0.39562373359998065, Validation Loss: 0.40056782960891724\n",
      "Epoch 990: Training Loss: 0.3955438087383906, Validation Loss: 0.4015328288078308\n",
      "Epoch 991: Training Loss: 0.39557212591171265, Validation Loss: 0.4014457166194916\n",
      "Epoch 992: Training Loss: 0.39557042717933655, Validation Loss: 0.40125784277915955\n",
      "Epoch 993: Training Loss: 0.3954897125562032, Validation Loss: 0.4010998010635376\n",
      "Epoch 994: Training Loss: 0.39558004836241406, Validation Loss: 0.40173962712287903\n",
      "Epoch 995: Training Loss: 0.3954849640528361, Validation Loss: 0.40136510133743286\n",
      "Epoch 996: Training Loss: 0.3955584118763606, Validation Loss: 0.4010199010372162\n",
      "Epoch 997: Training Loss: 0.39548008143901825, Validation Loss: 0.40164095163345337\n",
      "Epoch 998: Training Loss: 0.395739550391833, Validation Loss: 0.40218621492385864\n",
      "Epoch 999: Training Loss: 0.39546143511931103, Validation Loss: 0.4008685350418091\n",
      "Epoch 1000: Training Loss: 0.3955406993627548, Validation Loss: 0.4013226628303528\n"
     ]
    }
   ],
   "source": [
    "net = Net1(initial, final)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "a, b, c = fit(X2, X2_val, Y_p, Y_valp, net, optimizer, criterion, n_epochs, \n",
    "                    n_batches, batch_to_avg, lr, clipping, PATH, device, verbose)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a,'r', b, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\Data\\\\checkpoints\\\\mytraining3.pt'\n",
    "initial = X2.shape[1]\n",
    "final = int(round(initial * 1.5, 0)) \n",
    "device = torch.device('cpu')\n",
    "net = Net1(initial, final)\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3974575102329254\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_inputs = torch.FloatTensor(X2_val.values)\n",
    "    val_labels = torch.FloatTensor(Y_valp.values)\n",
    "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "    val_outputs = net.forward(val_inputs)\n",
    "    val_loss = criterion(val_outputs, val_labels) \n",
    "    print(val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_outputs = val_outputs.numpy()\n",
    "val_labels = val_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8018710680001002\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, auc_thresholds = roc_curve(val_labels, val_outputs)\n",
    "print(auc(fpr, tpr)) # AUC of ROC\n",
    "plot_roc_curve(fpr, tpr, 'recall_optimized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\Data\\\\dep')\n",
    "data2 = data[data.cost>0]\n",
    "data_val2 = data_val[data_val.cost>0]\n",
    "X_l = data2.iloc[:,1:]\n",
    "Y_l = data2.iloc[:,0]\n",
    "Xval_l = data_val2.iloc[:,1:]\n",
    "Yval_l = data_val2.iloc[:,0]\n",
    "\n",
    "ranking = np.load('ranking.npy')\n",
    "\n",
    "my_index = X_l.columns.values[ranking]\n",
    "X_l2 = X_l[my_index].copy()\n",
    "X_l2['intercept'] = 1\n",
    "\n",
    "Xval_l2 = Xval_l[my_index].copy()\n",
    "Xval_l2['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950487.0221827454"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(data2.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\prog_calc')\n",
    "from fit4_nn import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, initial, final):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(initial, final)\n",
    "        self.fc2 = nn.Linear(final, 1)                \n",
    "                   \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = X_l2.shape[1]\n",
    "final = int(round(initial * 1.5, 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "lr = 0.01\n",
    "verbose = 1\n",
    "n_batches = 6\n",
    "batch_to_avg = 2\n",
    "clipping = 0.5\n",
    "PATH = 'C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\Data\\\\checkpoints\\\\mytraining4.pt'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 950488.5989583334, Validation Loss: 1184869.125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2: Training Loss: 950488.3333333334, Validation Loss: 1184880.25\n",
      "Epoch 3: Training Loss: 950487.3958333334, Validation Loss: 1184891.25\n",
      "Epoch 4: Training Loss: 950486.3229166666, Validation Loss: 1184906.125\n",
      "Epoch 5: Training Loss: 950486.75, Validation Loss: 1184927.875\n",
      "Epoch 6: Training Loss: 950489.0885416666, Validation Loss: 1184945.875\n",
      "Epoch 7: Training Loss: 950487.1614583334, Validation Loss: 1184957.125\n",
      "Epoch 8: Training Loss: 950486.9270833334, Validation Loss: 1184963.0\n",
      "Epoch 9: Training Loss: 950487.0729166666, Validation Loss: 1184960.375\n",
      "Epoch 10: Training Loss: 950488.6875, Validation Loss: 1184956.625\n",
      "Epoch 11: Training Loss: 950487.6458333334, Validation Loss: 1184969.5\n",
      "Epoch 12: Training Loss: 950487.1145833334, Validation Loss: 1184967.5\n",
      "Epoch 13: Training Loss: 950487.6041666666, Validation Loss: 1184958.875\n",
      "Epoch 14: Training Loss: 950487.1510416666, Validation Loss: 1184959.625\n",
      "Epoch 15: Training Loss: 950487.4791666666, Validation Loss: 1184967.125\n",
      "Epoch 16: Training Loss: 950490.4895833334, Validation Loss: 1184984.25\n",
      "Epoch 17: Training Loss: 950486.9114583334, Validation Loss: 1184999.25\n",
      "Epoch 18: Training Loss: 950486.6666666666, Validation Loss: 1185012.75\n",
      "Epoch 19: Training Loss: 950487.046875, Validation Loss: 1185029.75\n",
      "Epoch 20: Training Loss: 950488.9114583334, Validation Loss: 1185058.25\n",
      "Epoch 21: Training Loss: 950487.59375, Validation Loss: 1185081.5\n",
      "Epoch 22: Training Loss: 950488.8177083334, Validation Loss: 1185096.125\n",
      "Epoch 23: Training Loss: 950488.5364583334, Validation Loss: 1185105.0\n",
      "Epoch 24: Training Loss: 950488.8177083334, Validation Loss: 1185113.25\n",
      "Epoch 25: Training Loss: 950489.4791666666, Validation Loss: 1185121.75\n",
      "Epoch 26: Training Loss: 950489.5729166666, Validation Loss: 1185120.75\n",
      "Epoch 27: Training Loss: 950489.359375, Validation Loss: 1185127.25\n",
      "Epoch 28: Training Loss: 950491.140625, Validation Loss: 1185133.75\n",
      "Epoch 29: Training Loss: 950490.6614583334, Validation Loss: 1185141.5\n",
      "Epoch 30: Training Loss: 950491.015625, Validation Loss: 1185135.125\n",
      "Epoch 31: Training Loss: 950490.3177083334, Validation Loss: 1185125.875\n",
      "Epoch 32: Training Loss: 950490.9947916666, Validation Loss: 1185122.0\n",
      "Epoch 33: Training Loss: 950489.703125, Validation Loss: 1185121.0\n",
      "Epoch 34: Training Loss: 950490.1927083334, Validation Loss: 1185127.5\n",
      "Epoch 35: Training Loss: 950490.0260416666, Validation Loss: 1185120.375\n",
      "Epoch 36: Training Loss: 950489.2552083334, Validation Loss: 1185112.75\n",
      "Epoch 37: Training Loss: 950489.109375, Validation Loss: 1185109.5\n",
      "Epoch 38: Training Loss: 950489.03125, Validation Loss: 1185104.75\n",
      "Epoch 39: Training Loss: 950489.828125, Validation Loss: 1185089.375\n",
      "Epoch 40: Training Loss: 950489.5677083334, Validation Loss: 1185081.25\n",
      "Epoch 41: Training Loss: 950488.640625, Validation Loss: 1185077.0\n",
      "Epoch 42: Training Loss: 950488.5598958334, Validation Loss: 1185076.375\n",
      "Epoch 43: Training Loss: 950488.8072916666, Validation Loss: 1185069.125\n",
      "Epoch 44: Training Loss: 950488.3645833334, Validation Loss: 1185066.75\n",
      "Epoch 45: Training Loss: 950489.5052083334, Validation Loss: 1185081.25\n",
      "Epoch 46: Training Loss: 950490.0989583334, Validation Loss: 1185098.75\n",
      "Epoch 47: Training Loss: 950488.9322916666, Validation Loss: 1185099.875\n",
      "Epoch 48: Training Loss: 950490.7447916666, Validation Loss: 1185112.375\n",
      "Epoch 49: Training Loss: 950491.6458333334, Validation Loss: 1185128.875\n",
      "Epoch 50: Training Loss: 950489.3697916666, Validation Loss: 1185139.0\n",
      "Epoch 51: Training Loss: 950489.90625, Validation Loss: 1185157.875\n",
      "Epoch 52: Training Loss: 950489.9791666666, Validation Loss: 1185174.625\n",
      "Epoch 53: Training Loss: 950492.765625, Validation Loss: 1185198.5\n",
      "Epoch 54: Training Loss: 950491.0416666666, Validation Loss: 1185218.875\n",
      "Epoch 55: Training Loss: 950495.4427083334, Validation Loss: 1185250.375\n",
      "Epoch 56: Training Loss: 950495.84375, Validation Loss: 1185270.5\n",
      "Epoch 57: Training Loss: 950496.0833333334, Validation Loss: 1185268.5\n",
      "Epoch 58: Training Loss: 950496.171875, Validation Loss: 1185259.625\n",
      "Epoch 59: Training Loss: 950495.5260416666, Validation Loss: 1185260.625\n",
      "Epoch 60: Training Loss: 950495.8958333334, Validation Loss: 1185254.0\n",
      "Epoch 61: Training Loss: 950494.96875, Validation Loss: 1185246.75\n",
      "Epoch 62: Training Loss: 950494.7291666666, Validation Loss: 1185234.0\n",
      "Epoch 63: Training Loss: 950493.3125, Validation Loss: 1185217.5\n",
      "Epoch 64: Training Loss: 950492.9427083334, Validation Loss: 1185207.875\n",
      "Epoch 65: Training Loss: 950492.46875, Validation Loss: 1185202.875\n",
      "Epoch 66: Training Loss: 950492.0729166666, Validation Loss: 1185187.625\n",
      "Epoch 67: Training Loss: 950493.703125, Validation Loss: 1185174.5\n",
      "Epoch 68: Training Loss: 950492.125, Validation Loss: 1185162.125\n",
      "Epoch 69: Training Loss: 950491.25, Validation Loss: 1185156.0\n",
      "Epoch 70: Training Loss: 950491.1614583334, Validation Loss: 1185157.75\n",
      "Epoch 71: Training Loss: 950490.8385416666, Validation Loss: 1185162.0\n",
      "Epoch 72: Training Loss: 950490.75, Validation Loss: 1185167.375\n",
      "Epoch 73: Training Loss: 950491.0677083334, Validation Loss: 1185164.125\n",
      "Epoch 74: Training Loss: 950490.8802083334, Validation Loss: 1185164.75\n",
      "Epoch 75: Training Loss: 950490.5260416666, Validation Loss: 1185172.5\n",
      "Epoch 76: Training Loss: 950490.9635416666, Validation Loss: 1185179.875\n",
      "Epoch 77: Training Loss: 950491.265625, Validation Loss: 1185178.125\n",
      "Epoch 78: Training Loss: 950491.53125, Validation Loss: 1185179.25\n",
      "Epoch 79: Training Loss: 950491.6302083334, Validation Loss: 1185187.125\n",
      "Epoch 80: Training Loss: 950493.4739583334, Validation Loss: 1185205.125\n",
      "Epoch 81: Training Loss: 950493.2786458334, Validation Loss: 1185211.125\n",
      "Epoch 82: Training Loss: 950493.1927083334, Validation Loss: 1185203.375\n",
      "Epoch 83: Training Loss: 950493.0989583334, Validation Loss: 1185209.625\n",
      "Epoch 84: Training Loss: 950492.7708333334, Validation Loss: 1185216.875\n",
      "Epoch 85: Training Loss: 950493.453125, Validation Loss: 1185214.75\n",
      "Epoch 86: Training Loss: 950493.9244791666, Validation Loss: 1185206.0\n",
      "Epoch 87: Training Loss: 950492.7135416666, Validation Loss: 1185196.875\n",
      "Epoch 88: Training Loss: 950491.5989583334, Validation Loss: 1185187.625\n",
      "Epoch 89: Training Loss: 950491.2395833334, Validation Loss: 1185178.875\n",
      "Epoch 90: Training Loss: 950491.5625, Validation Loss: 1185179.875\n",
      "Epoch 91: Training Loss: 950491.546875, Validation Loss: 1185173.25\n",
      "Epoch 92: Training Loss: 950490.859375, Validation Loss: 1185166.125\n",
      "Epoch 93: Training Loss: 950490.515625, Validation Loss: 1185157.875\n",
      "Epoch 94: Training Loss: 950491.3072916666, Validation Loss: 1185145.875\n",
      "Epoch 95: Training Loss: 950490.5208333334, Validation Loss: 1185144.25\n",
      "Epoch 96: Training Loss: 950490.3541666666, Validation Loss: 1185140.125\n",
      "Epoch 97: Training Loss: 950490.5260416666, Validation Loss: 1185139.125\n",
      "Epoch 98: Training Loss: 950489.6302083334, Validation Loss: 1185131.125\n",
      "Epoch 99: Training Loss: 950488.8229166666, Validation Loss: 1185118.125\n",
      "Epoch 100: Training Loss: 950489.2760416666, Validation Loss: 1185102.625\n",
      "Epoch 101: Training Loss: 950489.6171875, Validation Loss: 1185098.25\n",
      "Epoch 102: Training Loss: 950488.8489583334, Validation Loss: 1185102.5\n",
      "Epoch 103: Training Loss: 950489.1979166666, Validation Loss: 1185107.25\n",
      "Epoch 104: Training Loss: 950489.234375, Validation Loss: 1185108.0\n",
      "Epoch 105: Training Loss: 950489.2239583334, Validation Loss: 1185106.625\n",
      "Epoch 106: Training Loss: 950489.3385416666, Validation Loss: 1185107.375\n",
      "Epoch 107: Training Loss: 950488.90625, Validation Loss: 1185100.875\n",
      "Epoch 108: Training Loss: 950488.6354166666, Validation Loss: 1185099.0\n",
      "Epoch 109: Training Loss: 950488.4895833334, Validation Loss: 1185080.625\n",
      "Epoch 110: Training Loss: 950491.21875, Validation Loss: 1185061.125\n",
      "Epoch 111: Training Loss: 950487.9375, Validation Loss: 1185058.125\n",
      "Epoch 112: Training Loss: 950488.4114583334, Validation Loss: 1185063.25\n",
      "Epoch 113: Training Loss: 950488.1458333334, Validation Loss: 1185073.75\n",
      "Epoch 114: Training Loss: 950491.0286458334, Validation Loss: 1185083.625\n",
      "Epoch 115: Training Loss: 950488.984375, Validation Loss: 1185084.5\n",
      "Epoch 116: Training Loss: 950488.1614583334, Validation Loss: 1185092.375\n",
      "Epoch 117: Training Loss: 950488.75, Validation Loss: 1185099.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118: Training Loss: 950490.8125, Validation Loss: 1185112.75\n",
      "Epoch 119: Training Loss: 950489.734375, Validation Loss: 1185110.125\n",
      "Epoch 120: Training Loss: 950489.703125, Validation Loss: 1185100.875\n",
      "Epoch 121: Training Loss: 950488.765625, Validation Loss: 1185101.375\n",
      "Epoch 122: Training Loss: 950489.1770833334, Validation Loss: 1185103.875\n",
      "Epoch 123: Training Loss: 950489.4375, Validation Loss: 1185094.0\n",
      "Epoch 124: Training Loss: 950488.7083333334, Validation Loss: 1185094.0\n",
      "Epoch 125: Training Loss: 950488.9869791666, Validation Loss: 1185096.375\n",
      "Epoch 126: Training Loss: 950489.0989583334, Validation Loss: 1185105.25\n",
      "Epoch 127: Training Loss: 950489.2083333334, Validation Loss: 1185104.625\n",
      "Epoch 128: Training Loss: 950489.0520833334, Validation Loss: 1185101.875\n",
      "Epoch 129: Training Loss: 950488.6927083334, Validation Loss: 1185096.875\n",
      "Epoch 130: Training Loss: 950489.15625, Validation Loss: 1185081.875\n",
      "Epoch 131: Training Loss: 950489.4375, Validation Loss: 1185059.125\n",
      "Epoch 132: Training Loss: 950486.4375, Validation Loss: 1185044.625\n",
      "Epoch 133: Training Loss: 950488.6354166666, Validation Loss: 1185018.375\n",
      "Epoch 134: Training Loss: 950488.9166666666, Validation Loss: 1184992.375\n",
      "Epoch 135: Training Loss: 950486.65625, Validation Loss: 1184984.875\n",
      "Epoch 136: Training Loss: 950489.9479166666, Validation Loss: 1184968.0\n",
      "Epoch 137: Training Loss: 950486.921875, Validation Loss: 1184967.25\n",
      "Epoch 138: Training Loss: 950488.421875, Validation Loss: 1184973.75\n",
      "Epoch 139: Training Loss: 950488.5677083334, Validation Loss: 1184985.375\n",
      "Epoch 140: Training Loss: 950486.8385416666, Validation Loss: 1184992.125\n",
      "Epoch 141: Training Loss: 950486.6927083334, Validation Loss: 1185004.0\n",
      "Epoch 142: Training Loss: 950487.1458333334, Validation Loss: 1185009.875\n",
      "Epoch 143: Training Loss: 950488.2552083334, Validation Loss: 1185015.875\n",
      "Epoch 144: Training Loss: 950488.1614583334, Validation Loss: 1185014.25\n",
      "Epoch 145: Training Loss: 950487.2239583334, Validation Loss: 1185020.25\n",
      "Epoch 146: Training Loss: 950489.4583333334, Validation Loss: 1185040.375\n",
      "Epoch 147: Training Loss: 950486.828125, Validation Loss: 1185053.375\n",
      "Epoch 148: Training Loss: 950486.7447916666, Validation Loss: 1185070.0\n",
      "Epoch 149: Training Loss: 950488.4739583334, Validation Loss: 1185093.5\n",
      "Epoch 150: Training Loss: 950488.6197916666, Validation Loss: 1185117.75\n",
      "Epoch 151: Training Loss: 950489.59375, Validation Loss: 1185142.75\n",
      "Epoch 152: Training Loss: 950488.0260416666, Validation Loss: 1185164.125\n",
      "Epoch 153: Training Loss: 950490.765625, Validation Loss: 1185181.5\n",
      "Epoch 154: Training Loss: 950491.953125, Validation Loss: 1185191.375\n",
      "Epoch 155: Training Loss: 950491.8489583334, Validation Loss: 1185196.625\n",
      "Epoch 156: Training Loss: 950494.0989583334, Validation Loss: 1185212.625\n",
      "Epoch 157: Training Loss: 950493.8958333334, Validation Loss: 1185221.375\n",
      "Epoch 158: Training Loss: 950493.1875, Validation Loss: 1185215.375\n",
      "Epoch 159: Training Loss: 950493.7864583334, Validation Loss: 1185223.25\n",
      "Epoch 160: Training Loss: 950494.0260416666, Validation Loss: 1185236.25\n",
      "Epoch 161: Training Loss: 950494.5729166666, Validation Loss: 1185238.25\n",
      "Epoch 162: Training Loss: 950494.8958333334, Validation Loss: 1185237.875\n",
      "Epoch 163: Training Loss: 950493.921875, Validation Loss: 1185245.0\n",
      "Epoch 164: Training Loss: 950494.4322916666, Validation Loss: 1185247.125\n",
      "Epoch 165: Training Loss: 950494.5520833334, Validation Loss: 1185246.25\n",
      "Epoch 166: Training Loss: 950494.2552083334, Validation Loss: 1185233.375\n",
      "Epoch 167: Training Loss: 950495.1979166666, Validation Loss: 1185226.875\n",
      "Epoch 168: Training Loss: 950494.140625, Validation Loss: 1185219.375\n",
      "Epoch 169: Training Loss: 950493.71875, Validation Loss: 1185206.5\n",
      "Epoch 170: Training Loss: 950494.453125, Validation Loss: 1185189.875\n",
      "Epoch 171: Training Loss: 950493.0520833334, Validation Loss: 1185170.5\n",
      "Epoch 172: Training Loss: 950490.9791666666, Validation Loss: 1185163.75\n",
      "Epoch 173: Training Loss: 950491.0885416666, Validation Loss: 1185161.75\n",
      "Epoch 174: Training Loss: 950490.4895833334, Validation Loss: 1185167.5\n",
      "Epoch 175: Training Loss: 950490.2552083334, Validation Loss: 1185179.0\n",
      "Epoch 176: Training Loss: 950491.3177083334, Validation Loss: 1185184.375\n",
      "Epoch 177: Training Loss: 950492.59375, Validation Loss: 1185200.5\n",
      "Epoch 178: Training Loss: 950493.0520833334, Validation Loss: 1185214.625\n",
      "Epoch 179: Training Loss: 950494.265625, Validation Loss: 1185237.25\n",
      "Epoch 180: Training Loss: 950494.9739583334, Validation Loss: 1185250.75\n",
      "Epoch 181: Training Loss: 950495.6822916666, Validation Loss: 1185263.5\n",
      "Epoch 182: Training Loss: 950495.3125, Validation Loss: 1185275.375\n",
      "Epoch 183: Training Loss: 950497.8697916666, Validation Loss: 1185296.375\n",
      "Epoch 184: Training Loss: 950498.2135416666, Validation Loss: 1185308.75\n",
      "Epoch 185: Training Loss: 950498.7864583334, Validation Loss: 1185301.0\n",
      "Epoch 186: Training Loss: 950498.0520833334, Validation Loss: 1185292.875\n",
      "Epoch 187: Training Loss: 950496.6302083334, Validation Loss: 1185279.5\n",
      "Epoch 188: Training Loss: 950495.3489583334, Validation Loss: 1185262.5\n",
      "Epoch 189: Training Loss: 950496.359375, Validation Loss: 1185237.875\n",
      "Epoch 190: Training Loss: 950494.2239583334, Validation Loss: 1185217.75\n",
      "Epoch 191: Training Loss: 950495.0416666666, Validation Loss: 1185190.75\n",
      "Epoch 192: Training Loss: 950490.7395833334, Validation Loss: 1185173.25\n",
      "Epoch 193: Training Loss: 950490.5833333334, Validation Loss: 1185148.875\n",
      "Epoch 194: Training Loss: 950489.7760416666, Validation Loss: 1185138.375\n",
      "Epoch 195: Training Loss: 950489.9375, Validation Loss: 1185133.25\n",
      "Epoch 196: Training Loss: 950489.9322916666, Validation Loss: 1185127.0\n",
      "Epoch 197: Training Loss: 950489.671875, Validation Loss: 1185110.5\n",
      "Epoch 198: Training Loss: 950488.75, Validation Loss: 1185105.625\n",
      "Epoch 199: Training Loss: 950490.09375, Validation Loss: 1185114.125\n",
      "Epoch 200: Training Loss: 950489.3333333334, Validation Loss: 1185117.375\n",
      "Epoch 201: Training Loss: 950489.9166666666, Validation Loss: 1185113.5\n",
      "Epoch 202: Training Loss: 950488.984375, Validation Loss: 1185117.75\n",
      "Epoch 203: Training Loss: 950489.4947916666, Validation Loss: 1185132.25\n",
      "Epoch 204: Training Loss: 950489.984375, Validation Loss: 1185144.625\n",
      "Epoch 205: Training Loss: 950490.375, Validation Loss: 1185151.75\n",
      "Epoch 206: Training Loss: 950490.7916666666, Validation Loss: 1185149.875\n",
      "Epoch 207: Training Loss: 950492.4166666666, Validation Loss: 1185160.375\n",
      "Epoch 208: Training Loss: 950490.7135416666, Validation Loss: 1185161.0\n",
      "Epoch 209: Training Loss: 950491.328125, Validation Loss: 1185149.125\n",
      "Epoch 210: Training Loss: 950490.78125, Validation Loss: 1185152.75\n",
      "Epoch 211: Training Loss: 950492.25, Validation Loss: 1185162.5\n",
      "Epoch 212: Training Loss: 950491.5260416666, Validation Loss: 1185166.75\n",
      "Epoch 213: Training Loss: 950490.9166666666, Validation Loss: 1185153.625\n",
      "Epoch 214: Training Loss: 950491.09375, Validation Loss: 1185146.875\n",
      "Epoch 215: Training Loss: 950492.0572916666, Validation Loss: 1185158.125\n",
      "Epoch 216: Training Loss: 950490.8802083334, Validation Loss: 1185163.625\n",
      "Epoch 217: Training Loss: 950491.0729166666, Validation Loss: 1185165.75\n",
      "Epoch 218: Training Loss: 950491.6822916666, Validation Loss: 1185179.25\n",
      "Epoch 219: Training Loss: 950491.7395833334, Validation Loss: 1185181.875\n",
      "Epoch 220: Training Loss: 950491.8177083334, Validation Loss: 1185182.0\n",
      "Epoch 221: Training Loss: 950491.8385416666, Validation Loss: 1185179.25\n",
      "Epoch 222: Training Loss: 950492.078125, Validation Loss: 1185179.0\n",
      "Epoch 223: Training Loss: 950491.734375, Validation Loss: 1185162.375\n",
      "Epoch 224: Training Loss: 950493.265625, Validation Loss: 1185143.25\n",
      "Epoch 225: Training Loss: 950491.4375, Validation Loss: 1185122.0\n",
      "Epoch 226: Training Loss: 950489.4479166666, Validation Loss: 1185118.5\n",
      "Epoch 227: Training Loss: 950489.1302083334, Validation Loss: 1185113.125\n",
      "Epoch 228: Training Loss: 950489.0208333334, Validation Loss: 1185112.125\n",
      "Epoch 229: Training Loss: 950488.9270833334, Validation Loss: 1185108.375\n",
      "Epoch 230: Training Loss: 950489.3567708334, Validation Loss: 1185108.5\n",
      "Epoch 231: Training Loss: 950489.2447916666, Validation Loss: 1185111.0\n",
      "Epoch 232: Training Loss: 950489.3125, Validation Loss: 1185120.0\n",
      "Epoch 233: Training Loss: 950489.9375, Validation Loss: 1185133.875\n",
      "Epoch 234: Training Loss: 950489.6302083334, Validation Loss: 1185136.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235: Training Loss: 950489.7395833334, Validation Loss: 1185141.625\n",
      "Epoch 236: Training Loss: 950490.0416666666, Validation Loss: 1185156.875\n",
      "Epoch 237: Training Loss: 950491.7552083334, Validation Loss: 1185169.875\n",
      "Epoch 238: Training Loss: 950491.203125, Validation Loss: 1185167.75\n",
      "Epoch 239: Training Loss: 950492.484375, Validation Loss: 1185154.125\n",
      "Epoch 240: Training Loss: 950491.1822916666, Validation Loss: 1185147.25\n",
      "Epoch 241: Training Loss: 950490.3333333334, Validation Loss: 1185153.625\n",
      "Epoch 242: Training Loss: 950491.3802083334, Validation Loss: 1185160.125\n",
      "Epoch 243: Training Loss: 950490.8541666666, Validation Loss: 1185168.0\n",
      "Epoch 244: Training Loss: 950491.0364583334, Validation Loss: 1185171.75\n",
      "Epoch 245: Training Loss: 950491.1458333334, Validation Loss: 1185186.0\n",
      "Epoch 246: Training Loss: 950491.6927083334, Validation Loss: 1185189.25\n",
      "Epoch 247: Training Loss: 950492.40625, Validation Loss: 1185203.625\n",
      "Epoch 248: Training Loss: 950492.3958333334, Validation Loss: 1185206.875\n",
      "Epoch 249: Training Loss: 950492.8177083334, Validation Loss: 1185202.125\n",
      "Epoch 250: Training Loss: 950493.265625, Validation Loss: 1185196.5\n",
      "Epoch 251: Training Loss: 950491.9895833334, Validation Loss: 1185199.75\n",
      "Epoch 252: Training Loss: 950491.6458333334, Validation Loss: 1185209.5\n",
      "Epoch 253: Training Loss: 950492.5052083334, Validation Loss: 1185213.5\n",
      "Epoch 254: Training Loss: 950492.734375, Validation Loss: 1185206.375\n",
      "Epoch 255: Training Loss: 950492.40625, Validation Loss: 1185190.375\n",
      "Epoch 256: Training Loss: 950491.8072916666, Validation Loss: 1185171.625\n",
      "Epoch 257: Training Loss: 950490.4010416666, Validation Loss: 1185155.125\n",
      "Epoch 258: Training Loss: 950490.0625, Validation Loss: 1185141.125\n",
      "Epoch 259: Training Loss: 950491.7916666666, Validation Loss: 1185123.875\n",
      "Epoch 260: Training Loss: 950490.359375, Validation Loss: 1185114.125\n",
      "Epoch 261: Training Loss: 950489.8802083334, Validation Loss: 1185113.125\n",
      "Epoch 262: Training Loss: 950489.0677083334, Validation Loss: 1185110.75\n",
      "Epoch 263: Training Loss: 950488.8645833334, Validation Loss: 1185111.75\n",
      "Epoch 264: Training Loss: 950488.9895833334, Validation Loss: 1185119.625\n",
      "Epoch 265: Training Loss: 950488.4375, Validation Loss: 1185132.5\n",
      "Epoch 266: Training Loss: 950491.734375, Validation Loss: 1185148.25\n",
      "Epoch 267: Training Loss: 950490.625, Validation Loss: 1185152.5\n",
      "Epoch 268: Training Loss: 950491.0677083334, Validation Loss: 1185149.25\n",
      "Epoch 269: Training Loss: 950490.7213541666, Validation Loss: 1185144.125\n",
      "Epoch 270: Training Loss: 950490.1979166666, Validation Loss: 1185147.625\n",
      "Epoch 271: Training Loss: 950490.421875, Validation Loss: 1185147.625\n",
      "Epoch 272: Training Loss: 950491.21875, Validation Loss: 1185155.0\n",
      "Epoch 273: Training Loss: 950491.171875, Validation Loss: 1185157.375\n",
      "Epoch 274: Training Loss: 950490.6666666666, Validation Loss: 1185157.125\n",
      "Epoch 275: Training Loss: 950490.6302083334, Validation Loss: 1185154.625\n",
      "Epoch 276: Training Loss: 950490.3229166666, Validation Loss: 1185155.25\n",
      "Epoch 277: Training Loss: 950490.5729166666, Validation Loss: 1185158.5\n",
      "Epoch 278: Training Loss: 950490.921875, Validation Loss: 1185172.625\n",
      "Epoch 279: Training Loss: 950491.2916666666, Validation Loss: 1185190.0\n",
      "Epoch 280: Training Loss: 950491.3229166666, Validation Loss: 1185200.5\n",
      "Epoch 281: Training Loss: 950492.546875, Validation Loss: 1185200.75\n",
      "Epoch 282: Training Loss: 950492.671875, Validation Loss: 1185188.75\n",
      "Epoch 283: Training Loss: 950491.6614583334, Validation Loss: 1185187.25\n",
      "Epoch 284: Training Loss: 950492.0677083334, Validation Loss: 1185189.25\n",
      "Epoch 285: Training Loss: 950491.375, Validation Loss: 1185197.75\n",
      "Epoch 286: Training Loss: 950492.09375, Validation Loss: 1185201.25\n",
      "Epoch 287: Training Loss: 950492.6927083334, Validation Loss: 1185205.5\n",
      "Epoch 288: Training Loss: 950493.1614583334, Validation Loss: 1185202.5\n",
      "Epoch 289: Training Loss: 950492.5364583334, Validation Loss: 1185211.75\n",
      "Epoch 290: Training Loss: 950492.9791666666, Validation Loss: 1185211.375\n",
      "Epoch 291: Training Loss: 950492.8229166666, Validation Loss: 1185208.625\n",
      "Epoch 292: Training Loss: 950492.6302083334, Validation Loss: 1185199.25\n",
      "Epoch 293: Training Loss: 950492.28125, Validation Loss: 1185190.625\n",
      "Epoch 294: Training Loss: 950492.3385416666, Validation Loss: 1185191.625\n",
      "Epoch 295: Training Loss: 950492.125, Validation Loss: 1185189.375\n",
      "Epoch 296: Training Loss: 950491.6041666666, Validation Loss: 1185185.375\n",
      "Epoch 297: Training Loss: 950491.8177083334, Validation Loss: 1185184.75\n",
      "Epoch 298: Training Loss: 950493.203125, Validation Loss: 1185196.5\n",
      "Epoch 299: Training Loss: 950492.921875, Validation Loss: 1185203.125\n",
      "Epoch 300: Training Loss: 950492.3697916666, Validation Loss: 1185196.75\n"
     ]
    }
   ],
   "source": [
    "#net = Net1(initial, final)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)  #1184807.875\n",
    "\n",
    "PATH = 'C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\Data\\\\checkpoints\\\\mytraining4.pt'\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))\n",
    "    \n",
    "a, b, c = fit(X_l2, Xval_l2, Y_l, Yval_l, net, optimizer, criterion, n_epochs, \n",
    "                        n_batches, batch_to_avg, lr, clipping, PATH, device, verbose) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE3RJREFUeJzt3X+MXeV95/H3BztQtG3DDw9ZBDg2i1cqrLKUjIDV7kYoWYGNVjKJTOusVKyutd6mILXSrhSiSoVNslJYqUFKNz9EhRWDVhhKu8VSE1GL0LJ/lB/j4oAdRD350eCCMMSGJkpKavjuH/eZ5DLML88z5s7A+yVd3XO/5znPeR4f48/cc85wUlVIktTjlFEPQJK08hkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6rR71AN4ua9asqXXr1o16GJK0ouzbt+/lqhqbr927JkzWrVvHxMTEqIchSStKkr9bSDtPc0mSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKnbu+b3TBbrH/8RDh+GKnjjjZ+/Dy/P9n6y2sxmvnVT66eWT/TzbJKF1XosZX/LtS9467Ge6+/GYp+4faJjXswc345tHNfC2150EVx99Ynv40QYJvN46im44opRj0KSFu/Xf90wGbkLL4S77oJTThn8JHDKKW9enu39ZLWZes1mvnVT66f3tdDP08300/Fif2KezVL2t1z7mupv+JjP93djMT/NnuiYFzPHt2Mbx3Vi7U877cT3caIMk3msWQO/8RujHoUkLW9egJckdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSt3nDJMnOJEeSHBiqXZ/kYJI3kowP1dcl+UmS/e31laF1H0zydJLJJF9IklY/K8neJIfa+5mtntZuMslTSS4b6mtba38oybal+sOQJC3OQr6ZfBXYOK12APgY8MgM7b9dVZe2128N1b8M7AA2tNdUnzcDD1XVBuCh9hlg01DbHW17kpwF3AJcAVwO3DIVQJKk0Zg3TKrqEeDotNozVfXsQneS5Fzgl6vqr6uqgLuA69rqzcCutrxrWv2uGngUOKP1cw2wt6qOVtUxYC9vDTtJ0tvoZFwzWZ/kySR/leTft9p5wOGhNodbDeB9VfUCQHs/Z2ib52bYZra6JGlEVi9xfy8Aa6vqB0k+CPxZkkuAzNC25ulrtm0W3FeSHQxOkbF27dp5didJWqwl/WZSVa9V1Q/a8j7g28C/ZPDt4fyhpucDz7flF9vpq6nTYUda/TBwwQzbzFafaTx3VNV4VY2PjY31TE2SNIclDZMkY0lWteULGVw8/047ffXDJFe2u7huAB5om+0Bpu7I2jatfkO7q+tK4NXWz4PA1UnObBfer241SdKIzHuaK8k9wFXAmiSHGdxJdRT4Q2AM+PMk+6vqGuBDwKeTHAdeB36rqqYu3n+CwZ1hpwNfby+AzwH3JdkOfB+4vtW/BlwLTAI/Bn4ToKqOJvkM8ERr9+mhfUiSRiCDm6ve+cbHx2tiYmLUw5CkFSXJvqoan6+dvwEvSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6jZvmCTZmeRIkgNDteuTHEzyRpLxae0/lWQyybNJrhmqb2y1ySQ3D9XXJ3ksyaEk9yY5tdVPa58n2/p18+1DkjQaC/lm8lVg47TaAeBjwCPDxSQXA1uBS9o2X0qyKskq4IvAJuBi4OOtLcBtwO1VtQE4Bmxv9e3Asaq6CLi9tZt1HwudsCRp6c0bJlX1CHB0Wu2Zqnp2huabgd1V9VpVfReYBC5vr8mq+k5V/RTYDWxOEuDDwP1t+13AdUN97WrL9wMfae1n24ckaUSW+prJecBzQ58Pt9ps9bOBV6rq+LT6m/pq619t7WfrS5I0IksdJpmhVouoL6avtw4m2ZFkIsnESy+9NFMTSdISWOowOQxcMPT5fOD5OeovA2ckWT2t/qa+2vr3MjjdNltfb1FVd1TVeFWNj42NdUxLkjSXpQ6TPcDWdifWemAD8DjwBLCh3bl1KoML6HuqqoCHgS1t+23AA0N9bWvLW4BvtPaz7UOSNCKr52uQ5B7gKmBNksPALQy+IfwhMAb8eZL9VXVNVR1Mch/wLeA4cGNVvd76uQl4EFgF7Kyqg20XnwR2J/ks8CRwZ6vfCdydZLLtbyvAXPuQJI1GBj/sv/ONj4/XxMTEqIchSStKkn1VNT5fO38DXpLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktRt3jBJsjPJkSQHhmpnJdmb5FB7P7PVr0ryapL97fX7Q9tsTPJskskkNw/V1yd5rPV1b5JTW/209nmyrV83tM2nWv3ZJNcszR+FJGmxFvLN5KvAxmm1m4GHqmoD8FD7POX/VdWl7fVpgCSrgC8Cm4CLgY8nubi1vw24vfV1DNje6tuBY1V1EXB7a0fbbitwSRvXl1r/kqQRmTdMquoR4Oi08mZgV1veBVw3TzeXA5NV9Z2q+imwG9icJMCHgftn6Gt4H/cDH2ntNwO7q+q1qvouMNn6lySNyGKvmbyvql4AaO/nDK37N0m+meTrSS5ptfOA54baHG61s4FXqur4tPqbtmnrX23tZ+tLkjQiq5e4v78B3l9VP0pyLfBnwAYgM7StOeoscps3SbID2AGwdu3auUcuSVq0xX4zeTHJuQDt/QhAVf1DVf2oLX8NeE+SNQy+PVwwtP35wPPAy8AZSVZPqzO8TVv/Xgan22br6y2q6o6qGq+q8bGxsUVOVZI0n8WGyR5gW1veBjwAkOSft+saJLm89f8D4AlgQ7tz61QGF9D3VFUBDwNbpvc1bR9bgG+09nuAre1ur/UMvvk8vsh5SJKWwLynuZLcA1wFrElyGLgF+BxwX5LtwPeB61vzLcAnkhwHfgJsbQFwPMlNwIPAKmBnVR1s23wS2J3ks8CTwJ2tfidwd5JJBt9ItgJU1cEk9wHfAo4DN1bV6x1/BpKkThn8W//ONz4+XhMTE6MehiStKEn2VdX4fO38DXhJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktRtQWGSZGeSI0kODNXOSrI3yaH2fmarJ8kXkkwmeSrJZUPbbGvtDyXZNlT/YJKn2zZfSJLF7kOS9PZb6DeTrwIbp9VuBh6qqg3AQ+0zwCZgQ3vtAL4Mg2AAbgGuAC4HbpkKh9Zmx9B2GxezD0nSaCwoTKrqEeDotPJmYFdb3gVcN1S/qwYeBc5Ici5wDbC3qo5W1TFgL7Cxrfvlqvrrqirgrml9ncg+JEkj0HPN5H1V9QJAez+n1c8Dnhtqd7jV5qofnqG+mH28SZIdSSaSTLz00ksnPEFJ0sKcjAvwmaFWi6gvZh9vLlTdUVXjVTU+NjY2T5eSpMXqCZMXp04ttfcjrX4YuGCo3fnA8/PUz5+hvph9SJJGoCdM9gBTd2RtAx4Yqt/Q7ri6Eni1naJ6ELg6yZntwvvVwINt3Q+TXNnu4rphWl8nsg9J0gisXkijJPcAVwFrkhxmcFfW54D7kmwHvg9c35p/DbgWmAR+DPwmQFUdTfIZ4InW7tNVNXVR/xMM7hg7Hfh6e3Gi+5AkjUYGN1C9842Pj9fExMSohyFJK0qSfVU1Pl87fwNektTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1K0rTJL8TpIDSQ4m+d1WuzXJ3yfZ317XDrX/VJLJJM8muWaovrHVJpPcPFRfn+SxJIeS3Jvk1FY/rX2ebOvX9cxDktRn0WGS5F8B/wW4HPjXwH9MsqGtvr2qLm2vr7X2FwNbgUuAjcCXkqxKsgr4IrAJuBj4eGsLcFvrawNwDNje6tuBY1V1EXB7aydJGpHVHdv+CvBoVf0YIMlfAR+do/1mYHdVvQZ8N8kkgyACmKyq77R+dgObkzwDfBj4T63NLuBW4Mutr1tb/X7gfydJVVXHfGb29NPwa7+25N1K0ttm0yb4/OdP6i56wuQA8D+TnA38BLgWmAB+ANyU5Ib2+b9V1THgPODRoe0PtxrAc9PqVwBnA69U1fEZ2p83tU1VHU/yamv/8vAAk+wAdgCsXbt2cbM8/XT4wAcWt60kLQfvf/9J38Wiw6SqnklyG7AX+BHwTeA4g28OnwGqvf8B8J+BzNQNM59qqznaM8+64THeAdwBMD4+vrhvLRddBPfeu6hNJendousCfFXdWVWXVdWHgKPAoap6saper6o3gD/i56eyDgMXDG1+PvD8HPWXgTOSrJ5Wf1Nfbf172/4lSSPQezfXOe19LfAx4J4k5w41+SiD02EAe4Ct7U6s9cAG4HHgCWBDu3PrVAYX6fe06x8PA1va9tuAB4b62taWtwDfOCnXSyRJC9JzzQTgT9o1k38CbqyqY0nuTnIpg9NO3wP+K0BVHUxyH/AtBqfDbqyq1wGS3AQ8CKwCdlbVwdb/J4HdST4LPAnc2ep3Ane3i/hHGQSQJGlE8m75gX58fLwmJiZGPQxJWlGS7Kuq8fna+RvwkqRuhokkqZthIknqZphIkrq9ay7AJ3kJ+LuOLtYw7TfsV6h3yjzAuSxXzmV5Wuxc3l9VY/M1eteESa8kEwu5o2G5e6fMA5zLcuVclqeTPRdPc0mSuhkmkqRuhsnC3THqASyRd8o8wLksV85leTqpc/GaiSSpm99MJEndDJN5zPZ8+pUiyfeSPJ1kf5KJVjsryd4kh9r7maMe50yS7ExyJMmBodqMY8/AF9pxeirJZaMb+VvNMpdbk/x9Ozb7k1w7tO5TbS7PJrlmNKN+qyQXJHk4yTNJDib5nVZfccdljrmsxOPyC0keT/LNNpf/0errkzzWjsu97f/MTvu/t9/b5vJYknXdg6gqX7O8GPxfjL8NXAicyuABYBePelwnOIfvAWum1f4XcHNbvhm4bdTjnGXsHwIuAw7MN3YGT/r8OoMHp10JPDbq8S9gLrcC/32Gthe3v2unAevb38FVo55DG9u5wGVt+ZeAv23jXXHHZY65rMTjEuAX2/J7gMfan/d9wNZW/wrwibb828BX2vJW4N7eMfjNZG6X055PX1U/BXYzeP78SrcZ2NWWdwHXjXAss6qqR3jrQ89mG/tm4K4aeJTBg9XOZZmYZS6z2QzsrqrXquq7wCQ/f8jcSFXVC1X1N235h8AzDB6jveKOyxxzmc1yPi5VVT9qH9/TXgV8GLi/1acfl6njdT/wkSQzPcF2wQyTuf3sWfPN8HPoV4oC/iLJviQ7Wu19VfUCDP6DAs4Z2ehO3GxjX6nH6qZ2+mfn0OnGFTGXdmrkVxn8FLyij8u0ucAKPC5JViXZDxxh8Dj1bwOvVNXx1mR4vD+bS1v/KnB2z/4Nk7kt6Fnzy9y/rarLgE3AjUk+NOoBnSQr8Vh9GfgXwKXAC8AftPqyn0uSXwT+BPjdqvqHuZrOUFvuc1mRx6UGj0u/lMEjzi8HfmWmZu19yedimMxttufTrxhV9Xx7PwL8XwZ/yV6cOtXQ3o+MboQnbLaxr7hjVVUvtn8A3gD+iJ+fMlnWc0nyHgb/+P6fqvrTVl6Rx2WmuazU4zKlql4B/pLBNZMzkkw9UXd4vD+bS1v/XhZ+GnZGhsncZnw+/YjHtGBJ/lmSX5paBq4GDjCYw7bWbBvwwGhGuCizjX0PcEO7e+hK4NWp0y7L1bRrBx9lcGxgMJet7Y6b9cAG4PG3e3wzaefV7wSeqarPD61accdltrms0OMyluSMtnw68B8YXAN6GNjSmk0/LlPHawvwjWpX4xdt1HchLPcXg7tR/pbB+cffG/V4TnDsFzK4++SbwMGp8TM4N/oQcKi9nzXqsc4y/nsYnGb4JwY/SW2fbewMvrZ/sR2np4HxUY9/AXO5u431qfYf97lD7X+vzeVZYNOoxz80rn/H4HTIU8D+9rp2JR6XOeayEo/LB4An25gPAL/f6hcyCLxJ4I+B01r9F9rnybb+wt4x+BvwkqRunuaSJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTt/wNZOjOVwbPyWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a,'r', b, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:\\\\Users\\\\rober\\\\Desktop\\\\RAND_pro\\\\Data\\\\checkpoints\\\\mytraining4.pt'\n",
    "initial = X_l2.shape[1]\n",
    "final = int(round(initial * 1.5, 0)) \n",
    "device = torch.device('cpu')\n",
    "net = Net1(initial, final)\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184869.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_inputs = torch.FloatTensor(Xval_l2.values)\n",
    "    val_labels = torch.FloatTensor(Yval_l.values)\n",
    "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "    val_outputs = net.forward(val_inputs)\n",
    "    val_loss = criterion(val_outputs, val_labels) \n",
    "    print(val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_outputs = val_outputs.numpy()\n",
    "val_labels = val_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088.5168"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = np.sqrt(np.mean((val_labels - val_outputs)**2))\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
